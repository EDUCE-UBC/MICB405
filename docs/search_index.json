[["index.html", "Anvi’o binning and refinement tutorial 1 Introduction 1.1 Guide to the book 1.2 Brief introduction to ’omics methods 1.3 Some key aspects of metagenomics 1.4 Key points to be aware of and known pitfalls in metagenomics 1.5 Basic outline of a metagenomics pipeline", " Anvi’o binning and refinement tutorial Pranav Sampara 05/11/2020 1 Introduction This book is a brief guide to illustrate a typical binning and manual refinement workflow on the Anvi’o platform. This book uses four binning tools—METABAT2, MAXBIN2, CONCOCT, and DASTool. CheckM is used for MAG quality assessment and GTDB-Tk is used for taxonomic classification. Finally, TreeSAPP is used for functional annotation. 1.1 Guide to the book An introduction to ’omics methods is provided with a commentary and focus on metagenomics. Typical workflow, key points, and usual pitfalls for metagenomic analysis is provided Later, an introduction to the exercise that is to be done on Anvi’o is provided. This part of the book contains the project brief, data collection, and description of the data provided to students is discussed Next, a brief introduction and installation guide to Anvi’o and binning tools is provided Finally, example scripts are provided to achieve the final goal of high quality bins, taxonomic classification, and functional annotation of interested taxa. Clearly, this book is an introduction to binning and refinement. To maximize the benefit of this tutorial, please read the supporting scientific literature wherever provided and beyond. 1.2 Brief introduction to ’omics methods 1.3 Some key aspects of metagenomics A few key aspects that make metagenomics a valuable tool are: Ecophysiology: Enables the study of who is capable of doing what. In other words, function and identity relationships in the community or ecosystem of interest can be probed. Read Spang et al. 2019 Ecology: Estimate community characteristics – richness, diversity indices. Additionally, specific community functions can be investigated. Read Louca et al. 2016 Evolution: Finally, an investigation into how populations evolve in their natural habitat is possible with metagenomics. Particularly, selective pressures, if any, can be identified. Of course, you may need time-series sequencing and biogeochemistry data associated with the ecosystem of interest. Read Sun et al. 2016 1.4 Key points to be aware of and known pitfalls in metagenomics 1.5 Basic outline of a metagenomics pipeline Although there can be many means to make sense of the information in metagenomic datasets, a typical workflow consists of the following steps after generating sequencing data: Quality control, or polishing, of the raw reads Assembly of the raw reads into contiguous sequences, or contigs, and scaffolds Generation of coverage information by mapping the assembled reads to the polished reads Using sequence composition and coverage information to group, or bin, similar contigs. These bins are referred to as metagenome-assembled genomes or MAGs Quality control of the MAGs to assess completion and contamination Taxonomic classification and functional annotation of MAGs to identify who is present and what their metabolic potential is Phylogenomic or phylogenetic study of genomes or genes of interest and pangenomic analysis of similar taxa can be also be done to study the community networks "],["instructions.html", "2 Instructions 2.1 Goal of the exercise 2.2 Project ideas 2.3 Reports and Assessment", " 2 Instructions 2.1 Goal of the exercise The goal is to generate high quality metagenome assembled genomes (MAGs), identify the taxonomy, and the metabolic potential of the MAGs of interest. You will be working with metagenomic data from Cruise #72. You will be provided with prerequisite information, such as a merged profile database and contigs database, that you would use to bin MAGs and later classify the taxonomy. You would be working on an ’omics analysis and visualization platform, Anvi’o. For binning, you would use MetaBAT2, MaxBin2, CONCOCT, and DAS Tool. For taxonomic classification you would use GTDB-Tk and finally to check genome completeness and redundancy you would use CheckM. 2.2 Project ideas Determine what fractions of contigs map to MAGs vs contigs not in MAGs, as a way to evaluate the total amount of information available to evaluate Evaluate nitrogen cycling genes within MAGs using Prokka and TreeSAPP annotations -Compare taxonomic assignment TreeSAPP identified genes to GTDB-Tk assigned taxonomies of MAGs, generated using Anvi’o. Can you identify known groups based on published work -Evaluate changes in nitrogen cycling gene taxonomy and abundance at different depths in the Saanich inlet water column 2.3 Reports and Assessment Reports should be formatted as per the Instructions to Authors for the Journal of Bacteriology. Reports will account for 20% of the course marks. The project will be assessed by the following criteria: Completion of the proposed workflow. Expected outputs are available and formatted correctly (40%). A written report (4000 - 4500 words) adhering to the structure below (40-80%). 3. Oral presentation following the structure of the written report with emphasis on visual analysis of results (20%). Each group will complete one report with the sections listed below. The word counts for each section are recommnedations and can be adjusted by +/- 200 words, but the topics within each must be discussed! 2.3.1 Abstract (250 word max.) Concisely summarize the major results and implications of the report. 2.3.2 Importance (120 word max.) Lay explanation of the significance of the research performed to the field of environmental microbiology. 2.3.3 Introduction (1250 words) Introduce Saanich Inlet as a model ecosystem for studying microbial community responses to ocean deoxygenation e.g. seasonal cycles, relevant biogeochemistry, previous studies, etc. Provide relevant information regarding metagenomic approaches (data generation and application), pros, cons, and alternatives. Introduce one or all of: metagenomic binning, genome assembly, taxonomic profiling. Justify reasoning for using cultivation-independent methods. 2.3.4 Methods (750 words) Explain the workflow to generate inputs for the project to show you understand each step. Explain the tools used at each step, and why they were chosen, to answer your group’s specific question. Reference the bash and R(md) scripts containing all commands. Do not include the scripts themselves in the text of your manuscript but submit them as individual files on Canvas. Explain why each step was used, what the outputs are, etc. For example, metagenome assembly was used to build larger sequences (contigs) from the sequencing reads to reduce data size and redundancy, as well as inputs for binning. Don’t be silly and plagiarise this. Explain any potential deviations from the default parameters or the suggested workflow. 2.3.5 Results (1000 words) Provide a summary of the CheckM, and GTDB-Tk outputs, as well as those from other software, if applicable. Report on the Prokka annotations such as rRNA predicted, # genes annotated, and potential metabolisms of the medium- and high-quality bins. Report of the relevant results from your group’s selected analysis. 2.3.6 Discussion (1000 words) Relate the biological results back to the introduction and what is or is not sensible based on the geochemical state of the environment. How do the different population genome bins in your data set contribute to the metabolic pathways you investigated? (e.g. denitrification, dissimilatory sulfate reduction and oxidation). Do they encode partial or complete pathways (i.e. do your data support a distributed pathway)? Discuss your reasoning behind the steps you took to answer your group’s specific question. - Additional questions worth pursuing further. Briefly state any challenges encountered (we know there will be some!) and troubleshooting (parameter or software changes). 2.3.7 Figures and tables (≥4 with captions) Some recommended (not required!) figures to include: Geochemical gradients for nutrients (phosphate, silicate), nitrogen compounds (nitrate, nitrite, ammonia, nitrous oxide), oxygen and/or sulfur compounds. These are depth-dependent. Scatter plot comparing contamination % (Y-axis) versus completion % (X-axis) between population genome bins. Consider colour coding the bins by taxonomic rank e.g. phylum, order, etc. Indicate high, medium and low contamination bins (see Canvas for an example). RPKM bubble-plot of genes in a pathway versus taxonomy. 2.3.8 References (&gt;20) Necessary citations include: Each software used (including those in the provided bash scripts (MEGAHIT, MetaBAT, CheckM, etc.). Saanich Inlet data papers. 2.3.9 Fin. This evaluation rubric should serve as a useful scaffold for your group. How you expand on it will depend on your combined interests and analytic choices. Let the science take you somewhere! 2.3.10 UJEMI submission Outstanding reports will be invited to submit to The Undergraduate Journal of Experimental Microbiology and Immunology (UJEMI). More information on this will be provided near the end of the Finals Period. "],["background-information.html", "3 Background information 3.1 Project information 3.2 Data collection 3.3 Data availability", " 3 Background information 3.1 Project information Marine oxygen minimum zones (OMZ) are widespread areas of low dissolved oxygen (DO) in subsurface waters. Climate change resulting in increased stratification and reduced oxygen solubility in warming waters leads to an expansion of OMZ. Consequently, the microbial communities shift their metabolism to utilize alternative terminal electron acceptors to adapt to limiting DO conditions. This results in the release of climate active trace gases such as N2O and CH4. OMZs provide useful environmental contexts in which to study coupled biogeochemical cycling through microbial metabolic networks. Coupling ’omics data with biogeochemical data from the OMZs, the regulatory and response dynamics of microbial communities in the OMZs to changing DO levels can be studied. In this study, Saanich inlet, a seasonally anoxic fjord in British Columbia, was used as a model to study microbial community responses to changing DO levels. 3.2 Data collection Multi-omic sequence information from the Saanich Inlet along with geochemical data was collected over six years from the Saanich Inlet. For the context of this tutorial, 90 metagenomes, totalling 4.1 TB of cleaned reads or 16.2 GB of assembled data, were obtained. Please refer Hawley, A. K. et al. (2017) and Torres-Beltrán, M. et al. (2017) 3.3 Data availability "],["introduction-to-anvio.html", "4 Introduction to Anvi’o 4.1 What is Anvi’o 4.2 Anvi’o installation 4.3 Binning tools installation", " 4 Introduction to Anvi’o 4.1 What is Anvi’o Analysis and visualization platform for ’omics data, or Anvi’o is an open source platform for analyzing integrated multi-omics data developed by Eren et al. 2015. Anvi’o provides an excellent platform to visualize and analyze multi-omics data. We would use the platform to bin metagenome assembled genomes (MAGs) and manually refine the MAGs. Please visit Anvi’o webpage to learn more about Anvi’o and Anvi’o usage. We would then summarize, or output, the MAGs and perform quality control by estimating the completion and redundancy of the MAGs using CheckM developed by Parks et al. 2015. Later, we would classify the MAGs using GTDB-Tk developed by Chaumeil et al. 2020. 4.2 Anvi’o installation The following instructions provide an overview of commands that should work for most systems. For detailed instructions please see Anvi’o installation resources. Step 1: Please have a working install of miniconda on your terminal. Conda installation of Anvi’o is hassle-free and quick. Make sure the installation is updated by running conda update conda Step 2: For Mac OSX users # first get a copy of the following file (if you get a &quot;command not found&quot; # error for wget, you can either install it first, or manually download the # file from that URL: wget http://merenlab.org/files/anvio-conda-environments/anvio-environment-6.2.yml # just to make sure there is not a v6.2 environment already: conda env remove --name anvio-6.2 # create a new v6.2 environment using the file you just downloaded: conda env create -f anvio-environment-6.2.yml # activate that environment conda activate anvio-6.2 Linux / Windows users: conda create -y --name anvio-6.2 python=3.6 conda activate anvio-6.2 conda install -y -c conda-forge -c bioconda anvio==6.2 Step 3: Confirm installation anvi-self-test --suite mini If all is well, your browser should open and load the test run results 4.3 Binning tools installation While we are installing Anvi’o, we might as well install binning tools that we would use along with Anvi’o. Here we install - METABAT2 developed by Kang et al. 2019 MAXBIN2 developed by Wu et al. 2016 CONCOCT developed by Alneberg et al. 2014 DASTOOL developed by Sieber et al. 2018 # Metabat2 conda install metabat2 # Maxbin2 conda install maxbin2 # Concoct conda install -c bioconda concoct # DAS Tool conda install -c bioconda das\\_tool To verify if these binning tools are accessible by Anvi’o, please run these commands: conda activate anvio-6.2 anvi-cluster-contigs --help You should get a help menu on how to use the binning tools in Anvi’o. If there is a binning tool that is not accessible, you should see [NOT FOUND] next to the binning tool name. If you do not see this tex next to the three binning tools you installed, you are good to proceed. CheckM and GTDB-Tk should be installed on your terminal. "],["anvio-binning-exercise.html", "5 Anvi’o binning exercise 5.1 Automatic binning on Anvi’o using METABAT2, MAXBIN2, and CONCOCT 5.2 Assessing quality bins 5.3 Manual refinement 5.4 Iterative assessment and refinement", " 5 Anvi’o binning exercise You will be provided with contigs and merged profiles’ database to run binning and later manual refinement on the bins. Please see this tutorial on assembly-based metagenomics and metagenomic workflow tutorial on Anvi’o to understand how these files were obtained and an in-depth explanation on moving forward. The Anvi’o documentation also provides an excellent commentary on the nuances of each step. 5.1 Automatic binning on Anvi’o using METABAT2, MAXBIN2, and CONCOCT The following commands provide a step by step approach to obtain bins and to load the results on the visualization platform of Anvi’o on your browser for manual refinement. #Create variables for profile and contig databases for easy access to files profile_db = ~/path_to_profile_db contig_db = ~/path_to_contig_db #Make output directory Mkdir ~/path_to_output_directory #Create variable for output directory bin_out=~/path_to_output_directory #Operational parameters threads=32 #Change to suitable amount of threads #Clustering with metabat2 anvi-cluster-contigs -p $profile_db -c $contig_db -C metabat --driver metabat2 -T $threads --just-do-it #Clustering with concoct anvi-cluster-contigs -p $profile_db -c $contig_db -C concoct --driver concoct -T $threads --just-do-it #Clustering with maxbin2 anvi-cluster-contigs -p $profile_db -c $contig_db -C maxbin --driver maxbin2 -T $threads --just-do-it #Consensus binning with DASTOOL anvi-cluster-contigs -p $profile_db -c $contig_db --driver dastool -S metabat,concoct,maxbin -C dastool --just-do-it -T $threads #Obtain bins as fasta files anvi-summarize -p $profile_db -c $contig_db -C dastool -o $bin_out/dastool_out #Copy all bin fasta files for downstream processing such as quality estimation, taxonomic classification, and functional annotation #Make directory for fasta files for easy access mkdir $bin_out/fasta_files cp $bin_out/dastool_out/*/*fasta $bin_out/fasta_files 5.1.1 Note to EDUCE-TA’s In case we want the students to practice manual refinement alone, we can provide them with contaminated bins using Concoct. The following commands creates a fixed number of bins (ideally lower than the actual genomes expected) using Concoct profile_db = ~/path_to_profile_db contig_db = ~/path_to_contig_db #Make output directory Mkdir ~/path_to_output_directory #Operational parameters threads=32 #Change to suitable amount of threads bins=N #Change N to half of the number of bins expected to deliberately contaminate bins and later manually refine them #Clustering with concoct anvi-cluster-contigs -p $profile_db -c $contig_db -C concoct --driver concoct -T $threads --just-do-it --clusters $bins 5.2 Assessing quality bins Check completion and redundancy of MAGs using CheckM. Change paths and operational parameters as required for your system. For a more detailed understanding of how to use CheckM please visit CheckM wiki #Create checkM output directory mkdir ~/path_to_checkm_output_dir #Create variable for output directory checkm_out=~/path_to_checkm_output_dir #Establish bins’ fasta files directory in_fasta=~/path_to_output_fasta_files #Operational parameters – change as required on your system threads=48 pplacer_threads=24 #Run CheckM checkm lineage_wf -t $threads --pplacer_threads $pplacer_threads \\ -f $checkm_out/checkm_out.tsv --tab_table \\ -x fasta $in_fasta $checkm_out/checkm_lineage_wf #checkm_out.tsv is a tab separated file that provides quality estimates on the set of bins 5.2.1 Note the MAG quality Open the tab separated file, checkm_out.tsv in a spreadsheet viewer and study the completion and contamination column. We are looking for high/medium quality MAGs For a detailed commentary on high-quality MAGs, please refer Bowers et al. 2017 For this exercise, we aim to have bins with less than 10 % contamination and more than 50 % completion. We would now use the “anvi-refine” feature to manually refine the bins which have more than 10 % contamination. Please make a note of all the bins which are more than 50 % complete and have more than 10 % contamination. 5.3 Manual refinement Manual refinement on Anvi’o is described in great detail here 5.4 Iterative assessment and refinement After manual refinement of the bins on Anvi’o, iteratively run quality assessment and manual refinement of high contamination bins until you have a list of bins which are more than 90 % complete and less than 10 % contaminated. "],["processing-mags.html", "6 Processing MAGs 6.1 Taxonomic classification using GTDB-Tk", " 6 Processing MAGs Taxonomic classification can be done with GTDB-Tk. GTDB-Tk is a toolkit for using the Genome Taxonomy Database (GTDB) to classify taxonomy of the MAGs. Please read Parks et al. 2018 to learn more about GTDB and Chaumeil et al. 2020 for details on the GTDB-Tk tool. For an in-depth commentary on how to utilize GTDB-Tk please visit https://ecogenomics.github.io/GTDBTk/ After taxonomic classification, functional annotation can be done using TreeSAPP You will be implementing a pipeline called Tree-based Sensitive and Accurate Protein Profiler (TreeSAPP) for automated reconstruction of the nitrogen cycle along defined redox gradients in Saanich Inlet using the Google Cloud Platform. TreeSAPP takes either metagenomic or metatranscriptomic reads and aligns them to previously binned sequence data with each bin representing a putative microbial taxon. TreeSAPP determines three things: A. What taxa are in our metagenomic and metatranscriptomic data represented? B. Which marker genes do these taxa contain (metagenomic data) or actually express (metatranscriptomic data)? C. At what levels are those genes represented (metagenomic data) or expressed by the taxon? Please see the TreeSAPP wiki for more information on treesapp assign, the subcommand you will use. You will be provided with a script template for both the shell and R portion of your analysis (“treesapp_analysis.sh” and “treesapp_analysis.R”) that will guide you as you develop your code. 6.1 Taxonomic classification using GTDB-Tk The following commands will generate taxonomic classification of archaeal and bacterial MAGs #Create GTDB-Tk output directory mkdir ~/path_to_gtdbtk_output_dir #Create variable for output directory gtdbtk_out=~/path_to_gtdb_output_dir #Establish bins’ fasta files directory in_fasta=~/path_to_output_fasta_files #Export GTDB-Tk data path export GTDBTK_DATA_PATH=~/path_to_GTDB_data #Operational parameters – change as required on your system threads=48 pplacer_threads=24 #Run GTDB-Tk gtdbtk classify_wf -x fa --genome_dir $in_fasta \\ --out_dir $gtdbtk_out \\ --cpus $threads \\ --pplacer_cpus $pplacer_threads You would obtain two soft links in the gtdbtk output directory, each for bacteria and archaea taxonomic classification. Please follow the soft link to locate the actual files for taxonomic classification, and download to your system. "],["references.html", "7 References", " 7 References Eren, A. M. et al. Anvi’o: an advanced analysis and visualization platform for ’omics data. PeerJ 3, e1319 (2015). Kang, D. D. et al. MetaBAT 2: an adaptive binning algorithm for robust and efficient genome reconstruction from metagenome assemblies. PeerJ 7, e7359 (2019). Wu, Y.-W., Simmons, B. A. &amp; Singer, S. W. MaxBin 2.0: an automated binning algorithm to recover genomes from multiple metagenomic datasets. Bioinformatics 32, 605–607 (2016). Parks, D. H. et al. A standardized bacterial taxonomy based on genome phylogeny substantially revises the tree of life. Nat Biotechnol 36, 996–1004 (2018). Sun, C. L., Thomas, B. C., Barrangou, R. &amp; Banfield, J. F. Metagenomic reconstructions of bacterial CRISPR loci constrain population histories. The ISME Journal 10, 858–870 (2016). Louca, S. et al. Integrating biogeochemistry with multiomic sequence information in a model oxygen minimum zone. PNAS 113, E5925–E5933 (2016). Spang, A. et al. Proposal of the reverse flow model for the origin of the eukaryotic cell based on comparative analyses of Asgard archaeal metabolism. Nature Microbiology 4, 1138–1148 (2019). "]]
