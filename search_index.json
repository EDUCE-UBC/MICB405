[["index.html", "anvi’o binning and refinement tutorial 1 Introduction 1.1 Guide to the book 1.2 Some key features of metagenomic analysis 1.3 Basic outline of a metagenomics pipeline", " anvi’o binning and refinement tutorial Pranav Sampara, Steven Hallam, Stefanie Christen Sternagel, Julia Anstett, Stephan Koenig 16/11/2020 1 Introduction The following book provides a basic tutorial and background information for constructing metagenome assembled genomes (MAGs) with manual refinement using anvi’o. The tutorial leverages four binning tools—METABAT2, MAXBIN2, CONCOCT, and DASTool. CheckM is used for MAG quality assessment and GTDB-Tk is used for taxonomic classification. Finally, prokka is used for gene finding and annotation and TreeSAPP is used for fast phylogenetic mapping of functional anchor genes. Collectively, application of these tools is intended to help you answer the following foundational questions: What is the taxonomic and functional structure of the microbial community? How does this structure change along a gradient or in response to environmental perturbation? What are the ecological and biogeochemical consequences of this change? What are relevant units of selection, conservation or utilization within microbial communities? 1.1 Guide to the book An introduction to ’omics methods is provided with a commentary and focus on metagenomics. Typical workflows, key points, and common challenges are described. The next chapter of the book contains the capstone project description and grading rubric as well as information on the data sets to be used for analysis. A brief introduction and installation guide to anvi’o and binning tools follows based on information provided by the Meren lab at the University of Chicago. Finally, example scripts are provided to construct MAGs, conduct taxonomic classification, and complete functional annotations at the individual population and community levels of biological organization. Clearly, this book is a basic introduction to binning and refinement. To maximize the benefit of the tutorial provided, please read the supporting scientific literature wherever provided and look beyond for opportunities to exercise your scientific imagination. 1.2 Some key features of metagenomic analysis Environmental genomics, also known as metagenomics is the study of uncultivated microbial communities in natural and engineered environments using high-throughput sequencing and mass spectrometry methods. Application of metagenomic methods can provide insights into the ecophysiology, ecology and evolution of microorganisms at different levels of biological organization. At the same time these insights can help us uncover design principles for rational design of microbial communities and uncover a deep reservoir of sequence information useful in pathway engineering and biotechnology innovation. Ecophysiology: Enables us to identify who in the community of microorganisms is capable of doing what by linking taxonomic and functional information at the individual, population and community levels of biological organization. Read Spang et al. 2019 Ecology: Estimate community-level properties and interactions between organisms and their environment – richness, diversity, redundancy, degeneracy and resilience. Read Louca et al. 2016 Evolution: Investigate how individuals, populations and communities evolve in response to ecological and environmental state changes with an emphasis on defining ancestry and selective pressures in space and time. Read Sun et al. 2016 1.3 Basic outline of a metagenomics pipeline Metagenomic analysis is inherently heterogeneous given the complexity of the input data and the different types of processing steps available. However, certain common steps can be identified that define a conventional workflow consisting of the following processing steps: Quality control, or polishing, of the raw reads Assembly of the raw reads into contiguous sequences, or contigs, and scaffolds Generation of coverage information by mapping the assembled reads to the polished reads Using sequence composition and coverage information to group, or bin, similar contigs. These bins are referred to as metagenome-assembled genomes (MAGs) Quality control of the MAGs to assess completion and contamination Taxonomic classification and functional annotation of MAGs to identify who is present and what their metabolic potential is (see foundation questions in introduction) Metabolic pathway inference to predict given a set of genes what are the likely pathways present at different levels of biological organization. "],["instructions.html", "2 Instructions 2.1 Goal of the exercise 2.2 Project ideas 2.3 Reports and Assessment", " 2 Instructions 2.1 Goal of the exercise The primary objective of this tutorial is to use anvi’o to construct population genome bins, also known as metagenome assembled genomes (MAGs), to identify the taxonomy and metabolic potential of medium to high quality MAGs of interest Bowers et al. 2017. You will be working with metagenomic data sets from the Saanich Inlet time series Cruise #72. Prerequisite input files are provided to construct individual contig and merged profile databases for binning. As indicated above you will be using anvi’o, an ‘omics analysis and visualization platform’ developed by the Meren lab at the University of Chicago. Specific automated binning applications invoked by anvi’o can include MetaBAT2, MaxBin2, CONCOCT, and DAS Tool. Following MAG construciton you will use CheckM to check MAG completeness and contamination and GTDB-Tk to assign taxonomic affiliation. Collectively this information will enable you to reconstruct metabolic networks at the individual, population and community levels of biological organization. You will also be using prokka to predict and annotate open reading frames, and BWA to map reads onto MAGs to determine coverage and assess how representative the bins in the sampled community. Results from anvi’o will be compared to those generated using TreeSAPP, an application for fast phylogentic mapping of functional anchor genes, and the subject of a separate tutorial. 2.2 Project ideas Note that these are guiding ideas only. You are welcome to follow your own interests in developing a cogent reporting plan. You are working with primary scientific data and are encouraged to let the science “take you somewhere”. Determine what fraction of reads map to MAGs versus contigs not in MAGs. Are medium to high quality MAGs more or less abundant in the sample based on read mapping? Identify nitrogen cycling genes encoded in MAGs using prokka annotations and compare these results to TreeSAPP annotations using both MAGs and contigs. Are the results equivalent or different? Describe GTDB-Tk assigned taxonomies of MAGs. Can you identify previously described microbial groups based on the published record? Evaluate changes in nitrogen cycling gene taxonomy and abundance at different depths in the Saanich inlet water column with special emphasis on the modular denitrification pathway. How does the frequency distribution of denitrification genes change in relation to oxygen concentration and are there any gaps in pathway representation that are more likely to be filled by a specific taxonoic group? 2.3 Reports and Assessment Reports should be formatted as per the Instructions to Authors for the Journal of Bacteriology. These are roughly equivalent to the JEMI+ reporting guidelines (https://jemi.microbiology.ubc.ca/instructions/submission-guidelines). Reports will account for 20% of the course marks (50 points). The project will be assessed by the following criteria: Completion of the proposed workflow. Expected outputs are available and formatted correctly (40%). A written report (4000 - 4500 words) adhering to the structure below (40%). Five minute oral presentation following the structure of the written report with emphasis on visual analysis of results (20%). Each group will complete one report with the sections listed below. The word counts for each section are recommnedations and can be adjusted as needed, but the topics within each must needs be included! 2.3.1 Abstract (250 word max.) Concisely summarize the major results and implications of the report. 2.3.2 Importance (120 word max.) Lay explanation of the significance of the research performed to the field of environmental microbiology. 2.3.3 Introduction (1250 words) Introduce Saanich Inlet as a model ecosystem for studying microbial community responses to ocean deoxygenation e.g. seasonal cycles, relevant biogeochemistry, nitrogen cycle, relationship to oxygen minimum zones, previous studies, etc. Provide relevant information regarding metagenomic approaches (data generation and application), pros, cons, and alternatives. Introduce one or all of: metagenomic binning, genome assembly, taxonomic profiling. Justify reasoning for using cultivation-independent methods to study microbial metabolic networks e.g. nitrogen cycle. Here we describe “X, Y, Z”. End this section by telling the reader what you are going to tell them. 2.3.4 Methods (750 words) Explain the workflow to generate inputs for the project to show you understand each step. Explain the tools used at each step, and why they were chosen, to answer your group’s specific question. Reference the bash and R(md) scripts containing all commands. Do not include the scripts themselves in the text of your manuscript but submit them as individual files on Canvas. Explain why each step was used, what the outputs are, etc. For example, metagenome assembly was used to build contigs from the quality controlled sequencing reads and used as inputs for binning. Don’t be silly and plagiarise this. Explain any potential deviations from the default parameters or the suggested workflow. 2.3.5 Results (1000 words) Provide a summary of the binning results, CheckM, and GTDB-Tk outputs, as well as those from other software used, if applicable. Explain rationale for focusing on high-quality MAGs but provide information on the total number of bins constructed and their relative abundance based on read mapping. Report on the Prokka annotations such as rRNA predicted, # genes annotated, and potential metabolisms encoded in the medium to high-quality MAGs. Describe the relevant results from your group’s selected project ideas. Explain your experimental logic for each step in the analysis workflow. 2.3.6 Discussion (1000 words) Relate your results back to the introduction. Think about the environmental context and how the obseved taxonmy and function are related to measured geochemical conditions in the Saanich Inlet water column. Consider how each MAG contributes to different metabolic interactions (e.g. denitrification, sulfur oxidation, amino acid biosynthesis, etc). Can you find evidence for distributed metabolism? How do your results compare to previous studies? Are you able to make new observations that expand on prior knowledge? Briefly state any challenges encountered (we know there will be some!) and troubleshooting (parameter or software changes). Do new questions arise from your analysis? If you were to continue working on this proejct what future directions might you take to expand on the work or test specific hypotheses? 2.3.7 Figures and tables (≥4 with captions) Some recommended figures to include: Geochemical gradients for nutrients (phosphate, silicate), nitrogen compounds (nitrate, nitrite, ammonia, nitrous oxide), oxygen and/or sulfur compounds. These are depth-dependent. Scatter plot comparing contamination % (Y-axis) versus completion % (X-axis) between population genome bins. Consider colour coding the bins by taxonomic rank e.g. phylum, order, etc. Indicate high, medium and low contamination bins (see Canvas for an example). RPKM bubble-plot of genes in a pathway versus taxonomy. A diagram showing the taxonomic identity and abundance for each step in the denitrification pathway. A panel of phylogenetic trees showing the depth specific trends for selected TreeSAPP reference packages. 2.3.8 References (&gt;20) Necessary citations include: Each software application used (including those in the provided bash scripts (MEGAHIT, MetaBAT, CheckM, etc.). Make sure you include version numbers in your methods and indicate parameter settings used e.g. default. Relevant papers from Saanich Inlet or other marine oxygen minimum zones. 2.3.9 Fin. This evaluation rubric should serve as a useful scaffold for your group. How you expand on it will depend on your combined interests and analytic choices. Let the science take you somewhere! 2.3.10 UJEMI submission Outstanding reports will be invited to submit to The Undergraduate Journal of Experimental Microbiology and Immunology (UJEMI). "],["background-information.html", "3 Background information 3.1 Project information 3.2 Data collection 3.3 Data availability 3.4 Preliminary data processing", " 3 Background information 3.1 Project information Marine oxygen minimum zones (OMZ) are widespread areas of low dissolved oxygen (DO) in subsurface waters. Climate change resulting in increased stratification and reduced oxygen solubility in warming waters leads to an expansion of OMZ. Consequently as oxygen levels decline, the microbial communities inhabiting OMZ waters shift their metabolisms to utilize alternative terminal electron acceptors. This results in the production of climate active trace gases such as nitrous oxide (N2O) and methane (CH4). OMZs provide useful environmental contexts in which to study coupled biogeochemical cycling through microbial metabolic networks. By combining ’omics data with biogeochemical data from OMZs it becomes possible to evaluate regulatory and response dynamics of microbial communities to changing DO levels. In this study, Saanich inlet, a seasonally anoxic fjord on the coast of Vancouver Island British Columbia, was used as a model ecosystem to study microbial community responses to changing DO levels. 3.2 Data collection Multi-omic sequence information (DNA, RNA and proteins) from Saanich Inlet along with geochemical parameter information was collected over many years prodicing a time series record of recurring water column stratificaiton and deep water renewal. For this tutorial, 90 metagenomes spanning water colum redix gradients in space and time, totalling 4.1 TB of cleaned reads or 16.2 GB of assembled data, were prepared. (a) Oxygen concentration contour for CTD data (February 2008 onward)35 indicating 16 sampling depths for water column geochemistry and high-resolution (HR) DNA samples for SSU libraries (small black dots) and six major depths for large volume (LV) samples for meta-genomics, -transcriptomics, -proteomics and LV SSU libraries (large black dots). (b) Sample inventory from February 2006 to October 2014 indicating multi-omic datasets included in this manuscript (solid black), in previous publications (gray) and accompanying datasets currently undergoing processing and analysis (open gray). Please refer Hawley, A. K. et al. (2017) and Torres-Beltrán, M. et al. (2017) 3.3 Data availability The following files are available on Orca. # Directory with all files /projects/micb405/resources/metagenomics_data # Raw reads by depth /projects/micb405/resources/metagenomics_data/SI072_100m_raw_reads.fastq.gz /projects/micb405/resources/metagenomics_data/SI072_120m_raw_reads.fastq.gz /projects/micb405/resources/metagenomics_data/SI072_200m_raw_reads.fastq.gz # Deduplicated set of contigs from the combined 100, 120 and 200 m datasets /projects/micb405/resources/metagenomics_data/SI072_Dedupe_Contigs.fa # Directory with bins refined with anvi&#39;o and CheckM table /projects/micb405/resources/metagenomics_data/SI072_Bins_Refined_final # Directory with taxonomic classification of refined bins by GTDB-Tk /projects/micb405/resources/metagenomics_data/SI072_Bins_Ref_5_GTDB_r95 # Directory with taxonomic classification of refined bins by Centrifuge /projects/micb405/resources/metagenomics_data/Import_To_Contig_DB_files/Centrifuge_Taxonomy # Directory with Prokka output for MAGs /projects/micb405/resources/metagenomics_data/Import_To_Contig_DB_files/Prokka_Functions # BAM files and indices for data /projects/micb405/resources/metagenomics_data/bams 3.4 Preliminary data processing 3.4.1 Quality filtering of reads Quality filtering was performed with Trimmomtic (v.0.35) developed by Bolger et al., (2014). Please check Trimmomatic documentation for more details on the use of Trimmomatic. The following parameters were used for Trimmomatic, with the other parameters as defaults: LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36 3.4.2 Assembly Megahit (v.1.1.3), developed by Li et al., (2015), was used with the following parameters (the rest of the parameters being default): –k-min 27 –k-step 10 –min-contig-len 500 Please review megahit documentation for more details on the usage 3.4.3 Deduplication BBtools and dedupe.sh script were used to deduplicate contigs Please visit the deduplication guide from the Joint Genome Institute for more information. The following command generates deduplicated assemblies dedupe.sh in=SI072_Combined_contigs.fa out=SI072_Dedupe_Contigs.fa outd=duplicates.fa 3.4.4 Mapping Mapping was performed with bwa-mem (Li H. (2013)). Please check bwa-mem documentation for details on the tool’s usage bwa mem -t 4 SI072_Dedupe_Contigs.fa SI072_(depth_here)m_raw_reads.fastq.gz | samtools view -b | samtools sort -o SI072_(depth_here)m_Dedupe_contigs_sorted.bam samtools view -b -F 4 SI072_(depth_here)m_Dedupe_contigs_sorted.bam &gt; SI072_(depth_here)m_Dedupe_contigs_sorted.mapped.bam "],["introduction-to-anvio.html", "4 Introduction to anvi’o 4.1 What is anvi’o 4.2 anvi’o installation 4.3 Binning tools installation", " 4 Introduction to anvi’o 4.1 What is anvi’o Analysis and visualization platform for ’omics data (anvi’o) was developed by Eren et al. 2015 at the University of Chicago. Anvi’o is an open source platform for analyzing integrated multi-omics data and provides a useful and intiutive framework for constructing and curating metagenome assebled genomes (MAGs) In this tutorial we will use anvi’o to construct metagenome assembled genomes (MAGs) from the Saanich Inlet water column with manual refinement. Please visit anvi’o webpage to learn more about the platform and its usage. After constructing MAGs from several depth intervals you will perform quality control by estimating completion and contamination using CheckM developed by Parks et al. 2015. You will then classify medium to high-quality MAGs Bowers et al. 2017 using GTDB-Tk developed by Chaumeil et al. 2020. 4.2 anvi’o installation The following instructions based on documentation provided by the Meren lab provide an overview of commands that should work for most operating systems. For detailed instructions please see anvi’o installation resources. Step 1: Please have a working install of miniconda on your terminal. Conda installation of anvi’o is hassle-free and quick. Make sure the installation is updated by running conda update conda Step 2: For Mac OSX users # first get a copy of the following file (if you get a &quot;command not found&quot; # error for wget, you can either install it first, or manually download the # file from that URL: wget http://merenlab.org/files/anvio-conda-environments/anvio-environment-6.2.yml # just to make sure there is not a v6.2 environment already: conda env remove --name anvio-6.2 # create a new v6.2 environment using the file you just downloaded: conda env create -f anvio-environment-6.2.yml # activate that environment conda activate anvio-6.2 Linux / Windows users: conda create -y --name anvio-6.2 python=3.6 conda activate anvio-6.2 conda install -y -c conda-forge -c bioconda anvio==6.2 Step 3: Confirm installation anvi-self-test --suite mini If all is well, your browser should open and load the test run results 4.3 Binning tools installation While we are installing anvi’o, we might as well install binning tools needed to construct MAGs. Here we install - METABAT2 developed by Kang et al. 2019 MAXBIN2 developed by Wu et al. 2016 CONCOCT developed by Alneberg et al. 2014 DASTOOL developed by Sieber et al. 2018 # Make sure you have activated the anvi&#39;o enironment conda activate anvio-6.2 # Metabat2 conda install -c bioconda metabat2 # Maxbin2 conda install -c bioconda maxbin2 # Concoct conda install -c bioconda concoct # DAS Tool conda install -c bioconda das_tool To verify if these binning tools are accessible by anvi’o, please run these commands: conda activate anvio-6.2 anvi-cluster-contigs --help You should get a help menu on how to use the binning tools in anvi’o. If there is a binning tool that is not accessible, you should see [NOT FOUND] next to the binning tool name. If you do not see this text next to the three binning tools you installed, you are good to proceed. CheckM (https://anaconda.org/bioconda/checkm-genome) and GTDB-Tk (https://anaconda.org/bioconda/gtdbtk) should also be installed on your terminal. "],["anvio-binning-exercise.html", "5 anvi’o binning exercise 5.1 Automatic binning on anvi’o using METABAT2, MAXBIN2, and CONCOCT 5.2 Assessing quality MAGs 5.3 Manual refinement 5.4 Iterative assessment and refinement", " 5 anvi’o binning exercise You will be provided with fastq files, contig and merged profile databases to construct MAGs and conduct manual refinement. Please see this tutorial on assembly-based metagenomics and metagenomic workflow tutorial on anvi’o to understand how these files were obtained and an in-depth explanation on moving forward. The anvi’o documentation also provides an excellent commentary on the nuances of each step. Additional course material relevant to this tutorial can be found in the lecture slides describing metagenome assembled genomes. 5.1 Automatic binning on anvi’o using METABAT2, MAXBIN2, and CONCOCT The following commands provide a step by step approach to obtain MAGs and to load the results on the visualization platform of anvi’o on your browser for manual refinement. #Create variables for profile and contig databases for easy access to files profile_db = ~/path_to_profile_db contig_db = ~/path_to_contig_db #Make output directory Mkdir ~/path_to_output_directory #Create variable for output directory bin_out=~/path_to_output_directory #Operational parameters threads=32 #Change to suitable amount of threads #Clustering with metabat2 anvi-cluster-contigs -p $profile_db -c $contig_db -C metabat --driver metabat2 -T $threads --just-do-it #Clustering with concoct anvi-cluster-contigs -p $profile_db -c $contig_db -C concoct --driver concoct -T $threads --just-do-it #Clustering with maxbin2 anvi-cluster-contigs -p $profile_db -c $contig_db -C maxbin --driver maxbin2 -T $threads --just-do-it #Consensus binning with DASTOOL anvi-cluster-contigs -p $profile_db -c $contig_db --driver dastool -S metabat,concoct,maxbin -C dastool --just-do-it -T $threads #Obtain bins as fasta files anvi-summarize -p $profile_db -c $contig_db -C dastool -o $bin_out/dastool_out #Copy all bin fasta files for downstream processing such as quality estimation, taxonomic classification, and functional annotation #Make directory for fasta files for easy access mkdir $bin_out/fasta_files cp $bin_out/dastool_out/*/*fasta $bin_out/fasta_files 5.1.1 Note to EDUCE-TA’s In case we want the students to practice manual refinement alone, we can provide them with contaminated MAGs using Concoct. The following commands creates a fixed number of MAGs (ideally lower than the actual genomes expected) using Concoct profile_db = ~/path_to_profile_db contig_db = ~/path_to_contig_db #Make output directory Mkdir ~/path_to_output_directory #Operational parameters threads=32 #Change to suitable amount of threads bins=N #Change N to half of the number of bins expected to deliberately contaminate bins and later manually refine them #Clustering with concoct anvi-cluster-contigs -p $profile_db -c $contig_db -C concoct --driver concoct -T $threads --just-do-it --clusters $bins 5.2 Assessing quality MAGs Check completion and redundancy of MAGs using CheckM. Change paths and operational parameters as required for your system. For a more detailed understanding of how to use CheckM please visit CheckM wiki #Create checkM output directory mkdir ~/path_to_checkm_output_dir #Create variable for output directory checkm_out=~/path_to_checkm_output_dir #Establish bins’ fasta files directory in_fasta=~/path_to_output_fasta_files #Operational parameters – change as required on your system threads=48 pplacer_threads=24 #Run CheckM checkm lineage_wf -t $threads --pplacer_threads $pplacer_threads \\ -f $checkm_out/checkm_out.tsv --tab_table \\ -x fasta $in_fasta $checkm_out/checkm_lineage_wf #checkm_out.tsv is a tab separated file that provides quality estimates on the set of bins 5.2.1 Note the MAG quality Open the tab separated file, checkm_out.tsv in a spreadsheet viewer and study the completion and contamination column. We are looking for medium to high-quality MAGs Bowers et al. 2017. For this exercise, we aim to have bins with less than 10 % contamination and more than 50 % completion. We will now use the “anvi-refine” feature to manually refine the bins which have more than 10 % contamination. Please make a note of all the bins which are more than 50 % complete and have more than 10 % contamination. 5.3 Manual refinement Manual refinement on anvi’o is described in great detail here 5.4 Iterative assessment and refinement After manual refinement of the bins using anvi’o, iteratively run quality assessment and manual refinement of high contamination bins until you have a list of bins which are more than 90 % complete and less than 10 % contaminated. "],["processing-mags.html", "6 Processing MAGs 6.1 Taxonomic classification using GTDB-Tk", " 6 Processing MAGs Taxonomic classification can be done with GTDB-Tk. GTDB-Tk is a toolkit for using the Genome Taxonomy Database (GTDB) to classify taxonomy of the MAGs. Please read Parks et al. 2018 to learn more about GTDB and Chaumeil et al. 2020 for details on the GTDB-Tk tool. For an in-depth commentary on how to utilize GTDB-Tk please visit https://ecogenomics.github.io/GTDBTk/ After taxonomic classification, functional annotation can be done using different applications including prokka or TreeSAPP. Here, you will be implementing the Tree-based Sensitive and Accurate Protein Profiler (TreeSAPP) for automated reconstruction of the nitrogen cycle along defined redox gradients in Saanich Inlet. TreeSAPP takes either metagenomic or metatranscriptomic reads and aligns them to previously binned sequence data with each bin representing a putative microbial taxon. TreeSAPP determines three things: A. What taxa are represented in the metagenomic or metatranscriptomic sample? B. Which marker genes do these taxa encode (metagenomic data) or express (metatranscriptomic data)? C. At what levels are these genes present (metagenomic data) or expressed in the sample? Please see the TreeSAPP wiki for more information on treesapp assign, the subcommand you will use. You will be provided with a script template for both the shell and R portion of your analysis (“treesapp_analysis.sh” and “treesapp_analysis.R”) that will guide you as you develop your code. 6.1 Taxonomic classification using GTDB-Tk The following commands will generate taxonomic classification of archaeal and bacterial MAGs #Create GTDB-Tk output directory mkdir ~/path_to_gtdbtk_output_dir #Create variable for output directory gtdbtk_out=~/path_to_gtdb_output_dir #Establish bins’ fasta files directory in_fasta=~/path_to_output_fasta_files #Export GTDB-Tk data path export GTDBTK_DATA_PATH=~/path_to_GTDB_data #Operational parameters – change as required on your system threads=48 pplacer_threads=24 #Run GTDB-Tk gtdbtk classify_wf -x fa --genome_dir $in_fasta \\ --out_dir $gtdbtk_out \\ --cpus $threads \\ --pplacer_cpus $pplacer_threads You will obtain two soft links in the gtdbtk output directory, each for bacteria and archaea taxonomic classification. Please follow the soft link to locate the actual files for taxonomic classification, and download to your system. "],["protocol-and-scripts.html", "7 Protocol and scripts 7.1 Project files 7.2 Quality filtering and assembly 7.3 Deduplication of contigs 7.4 Prokka annotation 7.5 Mapping 7.6 Create anvi’o databases 7.7 Binning 7.8 Assess genome quality with CheckM and manual refinement 7.9 Assigning GTDB taxonomy to your bins 7.10 Make sure to echo the GTDB-Tk data pathecho 7.11 Working with TreeSAPP 7.12 TreeSAPP script", " 7 Protocol and scripts The following scripts and commands were used in the generation of the files available for data processing. In short, the protocol used is: Quality filter reads with Trimmomatic (v.0.35) (Bolger et al., (2014)). Assemble QC reads Megahit (v.1.1.3) (Li et al., (2015)) Annotate contigs with Prokka. Deduplicate contigs with BBtools (BBtools). Map the deduplicated contigs to the QC reads with bwa-mem (Li H. (2013)). Create anvi’o contigs and profile databases (Eren et al. 2015). Construct metagenome-assembled genomes (MAGs) using binning tools such as Metabat2 (Kang et al. 2019), Maxbin2 (Wu et al. 2016), Concoct (Alneberg et al. 2014), and DASTool (Sieber et al. 2018). Use CheckM (Parks et al. 2015) for estimating genome completion and contamination. Revisit anvi’o for manual refinement using the program anvi-refine Identify taxonomy using GTDB-Tk (Chaumeil et al. 2020). Downstream processing of MAGs using TreeSAPP (Morgan-Lang et al. 2020) to identify genes and functions of interest. 7.1 Project files In case you are stuck and need to access project files, please look through the file structure system, project_structure_level.txt. The path to the project files - /projects/micb405/resources/metagenomics_data/MICB405_Project_Files 7.2 Quality filtering and assembly Quality filtered reads and assembled contigs are available on the server. Quality filtering was performed using Trimmomatic and Assembly was performed using Megahit. 7.3 Deduplication of contigs dedupe.sh in=SI072_Combined_contigs.fa out=SI072_Dedupe_Contigs.fa outd=duplicates.fa 7.4 Prokka annotation If you want to leverage the anvi-import-function with Prokka annotations, you will need to do this first. Prokka takes a long time to run, but this needs to happen first before you had anything over to the students. This protocol follows tutorial listed here. Download and install the following: wget https://raw.githubusercontent.com/karkman/gff_parser/master/gff_parser.py -O gff_parser.py` pip install gffutils Before you begin, you’ll need to modify the Prokka code a little bit before running the tool to ensure you get partial gene calls from Prodigal since we’re working with metagenomes. For Prokka v1.14.6 you’ll want to take away the -c flag on line 717 of the prodigal command. Run Prokka on your deduplicated combined assembly: prokka --prefix PROKKA --outdir PROKKA --cpus 4 --metagenome SI072_Dedupe_Contigs.fa Prokka takes a really long time to run, but you only really need the .gff file that it produces. Once the job gets to the tbl2asn step, just kill it (it will make you cry inside, but it’ll be fine I promise! Have solace in knowing that there’s a good chance that it would have crashed on its own anyway). Next you’ll use that custom script you just downloaded and installed: python gff_parser.py PROKKA/PROKKA.gff --gene-calls gene_calls.txt --annotation gene_annot.txt This script parses the gff file to give you the inputs that you’ll need. However, as of anvio-6.2, this script produces an extra column in the gene-calls.txt file that is no longer accepted when you try to build the contig database (it’s column labeled call_type) and it’s the 7th one in from the left. The quickest way I dealt with it was to use the cut command: cut -f1,2,3,4,5,6,8,9 path/PROKKA/BAC_gene_calls.txt &gt; path/PROKKA/BAC_gene_call_fixed.txt 7.5 Mapping This step can happen in parallel with running Prokka or building your contigs database if you have the RAM, but as long as this happens before you make your profile databases, you’ll be fine. First, you need to index the input fasta file: bwa index SI072_Dedupe_Contigs.fa Then follow the following steps for each sample: bwa mem -t 4 SI072_Dedupe_Contigs.fa SI072_(depth_here)m_raw_reads.fastq.gz | samtools view -b | samtools sort -o SI072_(depth_here)m_Dedupe_contigs_sorted.bam samtools view -b -F 4 SI072_(depth_here)m_Dedupe_contigs_sorted.bam &gt; SI072_(depth_here)m_Dedupe_contigs_sorted.mapped.bam samtools index SI072_(depth_here)m_Dedupe_contigs_sorted.mapped.bam 7.6 Create anvi’o databases You would need to create a contigs database to store all information related to the contigs and a profile database to store sample specific information about the contigs. 7.6.1 Building contigs database anvi-gen-contigs-database -f SI072_Dedupe_Contigs.fa -o SI072_Dedupe_contigs.db --external-gene-calls Prokka_to_Anvio/BAC_gene_calls_fixed.txt -n 'SI072 100m,120m, 200m with Prokka' --ignore-internal-stop-codons anvi-import-functions -c SI072_Dedupe_contigs.db -i Prokka_to_Anvio/BAC_gene_annot.txt anvi-run-hmms -c SI072_Dedupe_contigs.db --num-threads 8 You could annotate the contigs with NCBI cogs, but since we used Prokka, we will skip any more annotation. 7.6.2 Building profile databases Since you’ve already gone through the trouble of building up your bam and bam.bai files, you can skip right ahead to the anvi-profile step, note that the minimum contig length cut-off is 2000 bp: anvi-profile -i SI072_(depth_here)m_Dedupe_contigs_sorted.mapped.bam -c SI072_Dedupe_contigs.db --min-contig-length 2000 --output-dir SI072_(Depth)_Profile --sample-name “sample_name” #Assign your sample name Do this for each sample. You could run a for loop to ease through Next, merge the profiles together: anvi-merge *Profile/PROFILE.db -o SI072_MERGED -c SI072_Dedupe_contigs.db --sample-name SI072 7.7 Binning Once anvio can see your binnning tools, it’s time to make some bins with concoct, maxbin2, and metabat2. Its very important that you set the minimum contig length is the same as your profile db, some of the binning algorithms fail if you don’t anvi-cluster-contigs -p SI072_MERGED/PROFILE.db -c SI072_Dedupe_contigs.db -C maxbin2 --driver maxbin2 -T 8 --min-contig-length 2000 --just-do-it anvi-cluster-contigs -p SI072_MERGED/PROFILE.db -c SI072_Dedupe_contigs.db -C maxbin2 --driver maxbin2 -T 8 --min-contig-length 2000 --just-do-it anvi-cluster-contigs -p SI072_MERGED_Profile/PROFILE.db -c SI072_Dedupe_contigs.db -C maxbin2 --driver maxbin2 -T 8 --min-contig-length 2000 --just-do-it Now we can use dastool to consolidate these bins: anvi-cluster-contigs -p SI072_MERGED_Profile/PROFILE.db -c SI072_Dedupe_contigs.db -C dastool --driver dastool -T 8 --just-do-it -S concoct,metabat2,maxbin2 Now that we’ve got some bins with dastool, it’s time to grab the fasta files. First, you need to make a summary directory: anvi-summarize -p SI072_MERGED_Profile/PROFILE.db -c SI072_Dedupe_contigs.db -o SI072_SUMMARY_MERGE_Pre_Ref -C dastool make a directory to put the fasta files to put your bins into mkdir SI072_Bins_Pre_Man Copy the fasta files into your new directory: cp SI072_SUMMARY_MERGE_Pre_Ref/bin_by_bin/*/*.fa SI072_Bins_Pre_Man 7.8 Assess genome quality with CheckM and manual refinement 7.8.1 CheckM usage checkm lineage_wf -t 8 --pplacer_threads 8 -f SI072_Bins_Pre_Man /SI072_Bins_ Pre_Man_CheckM.tsv --tab_table -x .fa SI072_Bins_Pre_Man/ SI072_Pre_Man/checkm_output Open the tab separated file, checkm_out.tsv in a spreadsheet viewer and study the completion and contamination column. We are looking for medium to high-quality MAGs Bowers et al. 2017. For this exercise, we aim to have bins with less than 10 % contamination and more than 50 % completion. We will now use the “anvi-refine” feature to manually refine the bins which have more than 10 % contamination. Please make a note of all the bins which are more than 50 % complete and have more than 10 % contamination. 7.8.2 Manual refinement Now, you’re good to download your contig database, merged profile directory, and CheckM table onto your local machine for further manual refinement since you’ll need the power of an internet browser to visually do the refinement. Make sure anvi’o is installed on that local machine. Also, keep an extra copy of these in case you make a mistake during manual refinement. Manual refinement on anvi’o is described in great detail here 7.8.3 Iterative assessment and refinement After manual refinement of the bins using anvi’o, iteratively run quality assessment and manual refinement of high contamination bins until you have a list of bins which are more than 90 % complete and less than 10 % contaminated. 7.8.4 Store bins on the server Once your done, repeat the following: anvi-summarize -p SI072_MERGED_Profile/PROFILE.db -c SI072_Dedupe_contigs.db -o SI072_SUMMARY_Ref_X -C dastool Make a directory to put the fasta files to put your bins into: mkdir SI072_Bins_Ref_X Copy the fasta files into your new directory: cp SI072_SUMMARY_MERGE_Ref_X /bin_by_bin/*/*.fa SI072_Bins_Ref_X Copy the directory SI072_Bins_Ref_X back to your server with: scp -r SI072_Bins_Ref_X username@server:/path 7.9 Assigning GTDB taxonomy to your bins Once you’ve got the final version of your bins on the server, you’re well positioned to assign taxonomy to your bins: 7.10 Make sure to echo the GTDB-Tk data pathecho \"export GTDBTK_DATA_PATH= /mnt/nfs/sharknado/LimsData/Hallam_Databases/formatted/GTDB-TK/release95/\" &gt; /miniconda3/envs/gtdbtk-1.3.0/etc/conda/activate.d/gtdbtk.sh gtdbtk classify_wf --genome_dir ~/SI072_Data/SI072_Dedupe/SI072_Bins_Ref_5/ --out_dir ~/SI072_Data/SI072_Dedupe/SI072_Bins_Ref_5_GTDB_r95/ -x .fa --cpus 4 7.11 Working with TreeSAPP Now that we have medium and high quality bins and their taxonomic identity, we move on to work with TreeSAPP for automated reconstruction of the nitrogen cycle. 7.12 TreeSAPP script Please use this script to work with TreeSAPP. The file paths are provided in the .csv file treesapp_paths.csv. Run this script from the same folder as your path file. !/bin/bash #activate conda treesapp environment prior to running w/ conda activate treesapp_cenv #set file reading in for path info paths_file=&quot;treesapp_paths.csv&quot; #make variable for ref pkg directory path refpkg_path=&quot;/projects/micb405/resources/metagenomics_data/MICB405_Project_Files/12_MICB405_refpkgs&quot; #make main directory for TreeSAPP outputs #corresponds to .csv main output directory #TreeSAPP will make individual iteration output subdirectories #w/in this main directory based on paths_file mkdir TreeSAPP_Outputs #change permissions so everyone has rwx privilege chmod -R 777 TreeSAPP_Outputs #loop through each line of paths_file #each line represents an interation of TreeSAPP #use fields in each line to run an iteration of TreeSAPP #$f1 is the assembly file (fasta). Please assign the variable f1 the path to the assembly fasta file #$f2 is the reads file (fastq). Please assign the variable f2 the path to the fastq file #$f3 is the output directory. Please assign the variable f3 the path to the output directory while IFS=, read -r f1 f2 f3 do #debugging echoes # echo &quot;assembly file:${f1}&quot; # echo &quot;reads file: ${f2}&quot; # echo &quot;output directory: ${f3}&quot; treesapp assign -i ${f1} -n 4 -m dna -r ${f2} -p pe -o ${f3} --refpkg_dir $refpkg_path --rpkm --trim_align --verbose --delete #change permissions on output subdirectories to rwx for everyone chmod -R 777 ${f3} done &lt; $paths_file ##remember to deactivate conda treesapp environment after running conda deactivate "],["references.html", "8 References", " 8 References Eren, A. M. et al. Anvi’o: an advanced analysis and visualization platform for ’omics data. PeerJ 3, e1319 (2015). Kang, D. D. et al. MetaBAT 2: an adaptive binning algorithm for robust and efficient genome reconstruction from metagenome assemblies. PeerJ 7, e7359 (2019). Wu, Y.-W., Simmons, B. A. &amp; Singer, S. W. MaxBin 2.0: an automated binning algorithm to recover genomes from multiple metagenomic datasets. Bioinformatics 32, 605–607 (2016). Parks, D. H. et al. A standardized bacterial taxonomy based on genome phylogeny substantially revises the tree of life. Nat Biotechnol 36, 996–1004 (2018). Sun, C. L., Thomas, B. C., Barrangou, R. &amp; Banfield, J. F. Metagenomic reconstructions of bacterial CRISPR loci constrain population histories. The ISME Journal 10, 858–870 (2016). Louca, S. et al. Integrating biogeochemistry with multiomic sequence information in a model oxygen minimum zone. PNAS 113, E5925–E5933 (2016). Spang, A. et al. Proposal of the reverse flow model for the origin of the eukaryotic cell based on comparative analyses of Asgard archaeal metabolism. Nature Microbiology 4, 1138–1148 (2019). "]]
