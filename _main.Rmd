---
title: "MICB 405 Bioinformatics"
subtitle: "2021.22"
author: "Axel Hauduc and Stephan Koenig"
date: "`r format(Sys.time(), '%B %d, %Y')`"
site: bookdown::bookdown_site
documentclass: book
bibliography: biblio/book.bib
csl: biblio/american-society-for-microbiology.csl
link-citations: true
description: >-
  A collection of tutorials and the capstone project information for MICB 405.
params:
  windows_version: 21H1
  windows_version_release_date: "the first half---May---of 2021"
  macos_version: Big Sur
  macos_version_link: https://support.apple.com/en-us/HT201475
  r_legacy_version: 3.3.3
  rstudio_version: 2021.09.0
  rstudio_legacy_version: 1.1.463
  project: deseq
---

```{r setup, include = FALSE}
library(here)
library(tidyverse)
```

# About {.unnumbered}

Welcome to the MICB 405 Tutorials website! You will find all tutorials and tutorial worksheets uploaded here each week ahead of the Friday tutorials.

MICB 405 introduces you to the concepts and applications of sequence-based bioinformatics research across several broad topic areas, including Unix/Linux and the command line and massively parallel sequencing and their applications. From a biological perspective, we will discuss the main considerations and applications of the computational tools used in these subject areas. In team projects supplementing lecture materials, you will work within groups to apply those bioinformatics tools to experimental data sets.

:::: {style="display: flex; align-items: center;"}

::: {.column width="10%"}
[![](`r here("images", "canvas.svg")`)](https://canvas.ubc.ca/courses/79973)
:::

::: {.column width="5%"}
:::

::: {.column width="85%"}
[Please visit the Canvas MICB 405 site](https://canvas.ubc.ca/courses/79973) for the syllabus, schedule (via calendar), lectures, module quizzes and team project details.
:::

::::

<!--chapter:end:index.Rmd-->

# (PART) Setup {-}

<!--chapter:end:Rmds/part_setup.Rmd-->

# Windows 10 {-}

## Updating the operating system {-}

```{r child = here("Rmds", "child_Rmds", "os_warning.Rmd")}
```

Please update to the latest version of Windows 10: version `r params$windows_version`.
(Major Windows versions indicate in which half of the year they were released, i.e. the latest version `r params$windows_version` in `r params$windows_version_release_date`.)

- Check your current Windows version ![](`r here("images", "one.png")`){width=3%}: Select the Windows Start button > Settings > System > About > scroll to bottom of page

  ![](`r here("images", "windows_version.png")`){width=75%}

- If you do not have the latest version installed: Select the Windows Start button > Settings > Update & Security > Windows Update

  ![](`r here("images", "windows_update.png")`){width=75%}

- If Windows Update does not offer the latest version, then manually update by going to [Windows 10 Downloads](https://www.microsoft.com/software-download/windows10) and choosing the latest version.

## Accessing a terminal to connect with the server {-}

These instructions will ensure that your local computer can connect to a Linux server and copy files between them using a software tool called OpenSSH.

-   OpenSSH comes pre-installed if you have Windows 10 version 1803 or newer. If you updated to the latest Windows 10 version, you should already have OpenSSH available to you. If your Windows 10 version is older because you cannot update, follow the [official Microsoft documentation on installing OpenSSH Client](https://docs.microsoft.com/en-us/windows-server/administration/openssh/openssh_install_firstuse).


-   The simplest way to access OpenSSH on Windows is to open the "Command Prompt" application, discoverable through the search bar. This application will provide sufficient functionality for all server-based work you will do in MICB405.


-   Once here, you can proceed to Tutorial 1.

## Additional considerations for Windows {-}

Be aware that when you open Command Prompt, you are using an entirely different local shell [with its own set of commands](https://docs.microsoft.com/en-us/windows-server/administration/windows-commands/windows-commands) unique from Linux. Only after you are connected to the server (using OpenSSH, see above) will you be using Bash. Tread with caution if executing commands other than `ssh` or `scp` from the tutorials while still in Command Prompt (i.e. while not connected to the server).

## Optional: Windows Subsystem for Linux {-}

You can create a true Linux environment at the command line by setting up [Windows Subsystem for Linux (WSL)](https://docs.microsoft.com/en-us/windows/wsl/install-win10). You can then access Bash on a virtualized Linux distribution, either directly through that distribution's Windows app or on Command Prompt to practice and run Linux commands on your machine without connecting to the server.


## Resources {-}

- [Which version of Windows operating system am I running?](https://support.microsoft.com/en-ca/help/13443/windows-which-version-am-i-running)
- [Update Windows 10](https://support.microsoft.com/en-ca/help/4027667/windows-10-update)


<!--chapter:end:Rmds/windows.Rmd-->

# macOS  {-}

## Updating the operating system {-}

```{r child = here("Rmds", "child_Rmds", "os_warning.Rmd")}
```

-   Please upgrade to the latest version of macOS, `r params$macos_version`.

-   Check your [current version of macOS](https://support.apple.com/en-ca/HT201260). If you do not have the latest version, follow the upgrade instructions on the [Apple support page](`r params$macos_version_link`).

-   If you cannot upgrade to the latest version, follow [these instructions](https://support.apple.com/en-us/HT211683) to find the system requirements and upgrade to latest versions available to you. If you need to maintain [32-bit compatibility](https://support.apple.com/en-us/HT208436) for any of your software, then upgrade to no higher than macOS 10.14 Mojave.

## Accessing a terminal to connect with the server {-}
-   macOS will come pre-installed with the Terminal app located at `/Applications/Utilities/Terminal.app`.

-   Once here, you can proceed to Tutorial 1.

## Resources {-}

- [Find out which macOS your Mac is using](https://support.apple.com/en-ca/HT201260)
- [How to upgrade to macOS Big Sur](https://support.apple.com/en-us/HT201475)
- [How to get old versions of macOS](https://support.apple.com/en-us/HT211683)
- [32-bit app compatibility with macOS High Sierra 10.13.4 and later](https://support.apple.com/en-us/HT208436)

<!--chapter:end:Rmds/macos.Rmd-->

# Linux  {-}

## Updating the operating system {-}

```{r child = here("Rmds", "child_Rmds", "os_warning.Rmd")}
```

-   It is always best practice to keep your software packages up to date on Linux as well.

-   On Debian-based (apt) distributions, a simple `sudo apt update` + `sudo apt upgrade` will do, but commands will vary on distributions.

## Accessing a terminal to connect with the server {-}

-   Use your preferred Terminal app. The location may vary depending on your distribution. On most GUIs, you should be able to find it by searching for "Terminal".

-   Once here, you can proceed to Tutorial 1.

<!--chapter:end:Rmds/linux.Rmd-->

# R and RStudio {-}

```{r include = FALSE, cache = FALSE}
# Path to install code for specific packages
packages_list <- here("R", "packages_micb405.R")
```

```{r child = here("Rmds", "child_Rmds", "r_and_rstudio_install.Rmd")}
```

<!--chapter:end:Rmds/r_and_rstudio.Rmd-->

# (PART) Tutorials {-}

<!--chapter:end:Rmds/part_tutorials.Rmd-->

# Overview {-}

## Formatting {-}

```{r child = here("Rmds", "child_Rmds", "formatting.Rmd")}
```

<!--chapter:end:Rmds/tutorials.Rmd-->

# Introduction & Linux Server Accounts

Welcome to the first MICB 405 lab! Today, we're going to get started with the essential skills and tools you'll be using for the rest of the course.

## Slides {-}

```{r echo=FALSE}
xaringanExtra::embed_xaringan(url = "slides/tutorials/bash.html",
                              ratio = "4:3")
```

You can download the slides for this tutorial below.

```{r echo = FALSE}
downloadthis::download_file(
  path         = here("slides", "tutorials", "bash.pdf"),
  output_name  = "introduction_and_linux_server_accounts",
  button_label = "Download slides"
)
```

## Logging in {-}

Once you have followed the setup steps and have opened Command Prompt (Windows) or Terminal (macOS, Linux), you should be ready to log in!

![](https://media2.giphy.com/media/LcfBYS8BKhCvK/giphy.gif?cid=ecf05e4747b1d69a24ea3b94dd23c9634105af0c7416ebb9&rid=giphy.gif){style="display: block; margin-left: auto; margin-right: auto;"}

Use the following command, replacing `<username>` with your actual username:

```{bash eval = FALSE}
ssh <username>@orca1.bcgsc.ca
```

Enter your password when prompted. Don't worry when characters don't appear - this is an intentional security feature.

You may have to respond to additonal prompts by typing in an answer (such as `yes`) and pressing <kbd>Enter</kbd>.

Let's break this command down: `ssh` is the command and stands for "secure shell". It allows users to log on to remote (opposite of "local", which is your computer in this instance) servers. All following text is the argument. There may be many arguments and each of these would be separated by spaces. `<username>` is mostly obvious, but crucially this positions your shell in the `home` directory of username on the server's system with the correct permissions. If everyone were to log on as `root` ("Administrator" in Windows-speak) this would be bad. `orca1.bcgsc.ca` (everything after the `@`) is known as the hostname or domain name and is the name of the device on the network it is connected to. Super-nerds sometimes replace this with the IP address.

## Bonus {-}

Review the manual pages for `ls`, `cd`, and `pwd` by typing `man` followed by the command. Can you use them to move through your local machine's folder system and find out where you are? How about the Orca server?

To exit the `man` page, press <kbd>q</kbd>.

### Delve into man pages and help {-}

1.  What is the difference between the `man` page, and `--help` or `-h` argument added to a command? When would you use either option? (try it on some commands, or look through StackExchange if you're confused)

1.  Here is a list of Bash commands often used in bioinformatics. Look up the `man` page for at least one of them. What do you think they do? Hypothetically, in what way do you think they could be used in a bioinformatics context? Which arguments would be the most handy? Discuss within your group!

    1.  `grep`
    1.  `tr`
    1.  `awk`

### Terminal keyboard shortcuts {-}

After typing some text, can you navigate, copy, and paste text without using the mouse? With some practice, this will be a lot faster.

```{r child = here("Rmds", "child_Rmds", "terminal_keyboard_shortcuts.Rmd")}
```

### Advanced {-}

Create a new file by running `nano testfile.txt` in your home directory or on your local machine (Use <kbd>ctrl</kbd> + <kbd>o</kbd> to save and <kbd>ctrl</kbd> + <kbd>x</kbd> to exit `nano`).

## Additonal resources {-}

If you complete these tasks, [check this page out for further reading and practice!](https://educe-ubc.github.io/MICB405/resources.html)!

<!--chapter:end:Rmds/bash.Rmd-->

# Bash and scripting

## Slides {-}

```{r echo=FALSE}
xaringanExtra::embed_xaringan(url = "slides/tutorials/scripts.html",
                              ratio = "4:3")
```

You can download the slides for this tutorial below.

```{r echo = FALSE}
downloadthis::download_file(
  path         = here("slides", "tutorials", "scripts.pdf"),
  output_name  = "bash_and_scripting",
  button_label = "Download slides"
)
```

## Exercise 1 {-}

Write a shell script that prints `Shell Scripting is Fun!` on the screen.
Now modify the shell script to include a variable. The variable will hold the contents of the message `Shell Scripting is Fun!`.

## Exercise 2 {-}

Write a shell script to check to see if the file `foobar` exists. If it does exist, display `"foobar" is definitely there`. Next, check to see if you can write to the file. If you can, display `You have permissions to edit "foobar"`. If you cannot, display `You do NOT have permissions to edit "foobar"`.

## Exercise 3 {-}

Write a shell script that displays `man`, `bear`, `pig`, `dog`, `cat`, and `sheep` on the screen with each appearing on a separate line. Try to do this in as few lines as possible i.e. not repeating commands excessively. Hint: you may have to create a "Bash array" of the words in the `for` line.

## Exercise 4 {-}

Write a shell script that takes the name of a file or directory (as an argument) and prints if it is a regular file, a directory, or another type of file on your screen.

## Exercise 5 {-}

Create a script that searches the files in

`/projects/micb405/data/bordetella/*` or  
`/projects/micb405/data/bordetella/Full_Run/*` 

for the following sequence

::: {style="overflow-wrap: break-word;"}
GCGCGCCTGGGCCCGGGCCTGCCCGCGATCGGCGCGGCGCACGATCAAGGGCATGGCGACATTGTCCAGCGCCGTGAACTCCGGCAGCAGGTGATGGAACTGGTAGACAAAGCCCAGGCTGCGATTGCGCAAGGCGCTCTTGCGGGATTCGGACAGGCCGTCGGCCGAGGTGCCGTCGACCACGACCGAGCCGCTGCTGGGCACATCCAGCAGGCCCAGGATGTGCAGCAGCGTGCTCTTGCCCGACCCC
:::

and produces the following (or similar) output:

```
ahauduc_mb20@orca01:~$ bash script.bash
F01_R1.fastq contains the sequence
F01_R1_1M.fastq does not contain the sequence
F01_R2.fastq does not contain the sequence
F01_R2_1M.fastq does not contain the sequence
```
 
## Additional resources {-}

For more help, check the [looping](https://swcarpentry.github.io/shell-novice/05-loop/index.html) and [scripting](https://swcarpentry.github.io/shell-novice/06-script/index.html) chapters of Software Carpentry!

<!--chapter:end:Rmds/scripts.Rmd-->

# FastQC

## Slides {-}

```{r, echo = FALSE, out.width = "100%", out.height = "388px"}
knitr::include_graphics("slides/tutorials/fastqc.pdf")
```

You can download the slides for this tutorial below.

```{r echo = FALSE}
downloadthis::download_file(
  path         = here("slides", "tutorials", "fastqc.pdf"),
  output_name  = "fastqc",
  button_label = "Download slides"
)
```

## Exercise 1 {-}

Navigate to `/projects/micb405/data/Ebola`. You should see paired-end FASTQ files. Let's say these files just came off the sequencer, and you want to take a quick look ensure nothing went awry during the sequencing process. You're not interested in the exact stats, but you still want to take a look at all the modules in the graphical HTML report.

Perform FastQC on these files, outputting the results to somewhere in your home directory.

Next, `scp` just the HTML files to your computer. (Hint: remember how `*` refers to any name of a file within the folder. How might you modify this to refer to files *ending* in `.html`?)

Now, find the transferred pair of HTML files on your computer and open them with the web browser of your choice!

## Exercise 2 {-}

Take a look at the different modules in the FastQC output. Check out "Per base sequence quality", "Per sequence quality scores", and "Per base sequence content" Describe why are these considered acceptable by FastQC. For "Per base sequence content", what would you need to change for FastQC to flag that module with a warning?

Take a look at "Per sequence GC content". What went wrong here? Would shifting this distribution so that the mean matched that of the theoretical mean be sufficient for FastQC to flag it as acceptable?

## Bonus {-}

Look at "Sequence Duplication Levels". This pattern is very distinct, and it's highly unusual for a whole-genome sequencing (DNA) run. However, let's say you know that this sequencing run was done perfectly. What kind of genetic or epigenetic material could have been sequenced here, rather than the simple DNA that FastQC expects, which causes some of the same sequences to appear many times?

Look at the `help` page for `fastp` and choose one trimming or filtering step. Apply this filtering to the Ebola FASTQs and output the filtered FASTQ to a directory in your home directory. When you run `fastqc` on these filtered FASTQs, can you observe the difference in the report stemming from the filtering steps you enacted? You can also use this [sandbox](https://fastq.sandbox.bio/) to play around with different `fastp` parameters!

## Additonal resources {-}

-   [FastQC tutorial and FAQ](https://rtsf.natsci.msu.edu/genomics/tech-notes/fastqc-tutorial-and-faq/)

-   [FastQC manual](https://dnacore.missouri.edu/PDF/FastQC_Manual.pdf)

<!--chapter:end:Rmds/fastqc.Rmd-->

# Working with SAMtools and BCFtools Part 1

## Slides {-}

```{r, echo = FALSE, out.width = "100%", out.height = "388px"}
knitr::include_graphics("slides/tutorials/samtools_bcftools.pdf")
```

You can download the slides for this tutorial below.

```{r echo = FALSE}
downloadthis::download_file(
  path         = here("slides", "tutorials", "samtools_bcftools.pdf"),
  output_name  = "bwa_samtools_and_bcftools",
  button_label = "Download slides"
)
```

## Main Exercise {-}

1. Create a new SAM file by aligning the following FASTQ files against your bordetella reference genome using `bwa mem` in a script.

    ```
    # FASTA
    /projects/micb405/analysis/references/ASM107827v1/GCA_001078275.1_ASM107827v1_genomic.fna
    
    # FASTQ
    /projects/micb405/data/bordetella/F01_R1_1M.fastq
    /projects/micb405/data/bordetella/F01_R2_1M.fastq
    ```

1. Run `samtools flagstat` on your resulting SAM file. What do the different lines mean?

1. Next, create a script that processes the resulting SAM into an indexed, sorted BAM with duplicates removed.

1. For sorting and duplicate removal, what is a "sanity check" step you could perform on the output (hint... what function within `samtools` can allow you to check your binary BAM file by eye) to verify that reads have been sorted (easier), or that duplicates have been removed (harder)? What would you be looking for to confirm that your commands worked as intended?

1. Then, add a line in your script to call variants using `bcftools`! This may take a while, so how could you run it so that it happens in the background and you could check back on it later?

## Bonus {-}
Download your reference, `bam`, `bam.bai`, `vcf.gz`, and `vc.gz.tbi` files and view them in IGV viewer. Go to one of the areas marked as a variant by `bcftools call`. Do you agree with this "call"?

## Additional resources {-}

[BWA manual pages](http://bio-bwa.sourceforge.net/bwa.shtml)

[SAMtools manual pages](http://www.htslib.org/doc/samtools.html)

[BCFtools manual pages](http://www.htslib.org/doc/bcftools.html)

[Introduction to the BAM file format](https://github.com/davetang/learning_bam_file)

[Introduction to the VCF file format](https://github.com/davetang/learning_vcf_file)

[IGV download](https://software.broadinstitute.org/software/igv/download)

[IGV user guide](http://software.broadinstitute.org/software/igv/UserGuide)

<!--chapter:end:Rmds/samtools_bcftools.Rmd-->

# Working with SAMtools and BCFtools Part 2

## Slides {-}

```{r, echo = FALSE, out.width = "100%", out.height = "388px"}
knitr::include_graphics("slides/tutorials/samtools_bcftools_part_2.pdf")
```

You can download the slides for this tutorial below.

```{r echo = FALSE}
downloadthis::download_file(
  path         = here("slides", "tutorials", "samtools_bcftools_part_2.pdf"),
  output_name  = "bwa_samtools_and_bcftools_part_2",
  button_label = "Download slides"
)
```

## Exercise {-}

1. Create a new alignment using `bwa aln`.
1. Do a quick comparison using the SN metrics from `samtools stats` of the `bwa aln` BAM and the previous `bwa mem` BAM. What is one metric that changed the most and why might this be?
1. Filter the `bwa aln` alignment in order to only keep reads with a mapping quality above a threshold of your choice.
    - Do a quick side-by-side comparison of coverage of your new and old `bwa aln` alignments using `bwa stats`. How does introducing a quality threshold lead to a difference here? Can you explain the pros and cons of filtering by mapping quality in terms of [sensitivity and specificity](https://en.wikipedia.org/wiki/Sensitivity_and_specificity)?
    - Choosing an arbitrary area along *Bordetella*'s single contig, what is one difference you can spot between your new and old `bwa aln` alignments besides read depth? (Hint: take a look at bit flags and positions, and [consult IGV's coloring scheme](http://software.broadinstitute.org/software/igv/interpreting_insert_size).)

## Additional resources {-}

[BWA manual pages](http://bio-bwa.sourceforge.net/bwa.shtml)

[SAMtools manual pages](http://www.htslib.org/doc/samtools.html)

[BCFtools manual pages](http://www.htslib.org/doc/bcftools.html)

[Introduction to the BAM file format](https://github.com/davetang/learning_bam_file)

[Introduction to the VCF file format](https://github.com/davetang/learning_vcf_file)

[IGV download](https://software.broadinstitute.org/software/igv/download)

[IGV user guide](http://software.broadinstitute.org/software/igv/UserGuide)

<!--chapter:end:Rmds/samtools_bcftools_part_2.Rmd-->

# ChIP-seq analysis

## Slides {-}

```{r, echo = FALSE, out.width = "100%", out.height = "388px"}
knitr::include_graphics("slides/tutorials/chipseq.pdf")
```

You can download the slides for this tutorial below.

```{r echo = FALSE}
downloadthis::download_file(
  path         = here("slides", "tutorials", "chipseq.pdf"),
  output_name  = "chipseq",
  button_label = "Download slides"
)
```

## Setup

1. Make sure you have [IGV](http://software.broadinstitute.org/software/igv/download) installed on your laptop

1. Check the `macs2` is installed and in your `$PATH` by running `which macs2`

1. If you do not see the folder where `macs2` is installed, run:

    ```{bash eval = FALSE}
    pip3 install macs2
    pip3 install --upgrade numpy
    ```

1. Set up a new folder to work with in the tutorial:

    ```{bash eval = FALSE}
    mkdir ~/ChIP_tutorial
    cd ~/ChIP_tutorial
    ```

1. Check the files needed for the tutorial using `ls -lh /projects/micb405/data/mouse/chip_tutorial`. These are the ChIP-seq reads that constitute the main input of your analysis.

## Suggested workflow

The (recommended) tools that you will working with to complete this project include:

- BWA
- SAMtools
- MACS2
- IGV (installed on your computer)

## Aligning using BWA-MEM

We will now process and map the reads using `bwa mem`.  Note the run time command syntax for `bwa mem` is different (easier and faster) from `bwa aln` introduced in class. The alignment process will take ~20 min to complete depending on server load. It is highly recommended to construct your commands within a shell script and generated associated log files by redirecting STDOUT as described in class. To make your subsequent commands easier, remember that you can define paths to commonly used files/resources in your script or shell as variables. For example, you can define the path to where the indexed mm10 genome is and the directory to where the tutorial input files are stored (see below). Best practice is to use the absolute rather than relative path when defining these variables, as that will ensure that your script will behave similarly no matter where it is located on your filesystem.

```{bash eval = FALSE}
GENOME=/projects/micb405/analysis/references/mm10/mm10.fa
DATA=/projects/micb405/data/mouse/chip_tutorial
```

You only need to `export` a variable if you want to add it to your current shell and have it persist between sessions. Since a script runs a subshell in a continuous session with its own unique variables, it's not necessary to `export` variables within it.

As should now be familiar, once defined you can call these paths in your script/shell by entering the special character `$` followed by the variable name as shown below. The following command will run `bwa mem` and stream the `sam` file into `samtools view`, which will then automatically convert it into a `bam` file. You can use this or design your own.

```{bash eval = FALSE}
# Input a.k.a. control alignment
bwa mem \
  -t 8 \
  $GENOME \
  $DATA/Naive_Input_1.fastq \
  $DATA/Naive_Input_2.fastq \
  | samtools view -h -b - -o Naive_Input.bam

# Treatment a.k.a. immunoprecipitated a.k.a. IP alignment
bwa mem \
  -t 8 \
  $GENOME \
  $DATA/Naive_H3K27ac_1.fastq \
  $DATA/Naive_H3K27ac_2.fastq \
  | samtools view -h -b - -o Naive_H3K27ac.bam
```

Note that the `-t 8` is to do multithreaded processing to improve speed. The `$GENOME` specifies the location of reference genome to use. The `$DATA/Naive_H3K27ac_1.fastq` `$DATA/Naive_H3K27ac_2.fastq` specifies the location of the reads. This step will take some time. Expect the program to run for about 20 mins or longer depending on the server load. You can add `nohup` in front of each command, or in in front of your command running the script with these commands, in order to assure that if you are disconnected from the server, these analysis steps do not stop.

## Check files

At the end, you should have something similar to:

```{bash eval = FALSE}
user01@orca01:~/ChIP_tutorial$ ls -lh
total 8.7G
-rw-r--r-- 1 mhirst orca_users 1.2G Oct  4 05:33 Naive_H3K27ac.bam
-rw-r--r-- 1 mhirst orca_users 1.2G Oct  4 05:36 Naive_Input.bam
-rw------- 1 mhirst orca_users   58 Oct  4 05:36 nohup.out
```

## Sort, mark duplicates, and index alignments using SAMtools

The output from `bwa mem` is an unsorted SAM file. Downstream tools require a sorted BAM file as input, so an intermediate step is to sort and index the alignment to facilitate future steps, similar to what was covered in our samtools/bcftools tutorial.

```{bash eval = FALSE}
# Tag mates
samtools fixmate -m Naive_Input.bam Naive_Input.fixmate.bam
# Position sort for markdup
samtools sort Naive_Input.fixmate.bam -o Naive_Input.fixmate.sort.bam
# Mark duplicates. Note that you don't have to remove them as this is done by MACS2 automatically
samtools markdup -S Naive_Input.fixmate.sort.bam Naive_Input.markdup.bam
# Sort again
samtools sort Naive_Input.markdup.bam -o Naive_Input.final.bam
# Index BAM
samtools index Naive_Input.final.bam
```

Repeat this for the treatment file as well.

## Clean-up intermediate files

Now is the time to clean-up any intermediate files that are not needed downstream, keeping only the `.final.bam` files needed by MACS2. When you are finished cleaning your files the working directory should be ~2.5G total.

## Call peaks using MACS2

Before doing peak calling, it is necessary to have a sample data set, as well as a control data set. While MACS2 offers the option to perform both regular (narrow) and broad peak-calling, in this case, we will only be doing regular peak calling, however the instructions are similar for broad peak-calling just add the additional flag `--broad` to the command.

We will call peaks on sample `Naive_H3K27ac.final.bam` and we will be using the `Naive_Input.final.bam` as a control (background). Once you have the sorted, duplicate-marked `bam` files for all samples, you can perform the MACS2 peakcalling like this:

```{bash eval = FALSE}
macs2 callpeak \
  -t Naive_H3K27ac.final.bam \
  -c Naive_Input.final.bam \
  -f BAMPE \
  -g mm \
  -n Naive_H3K27ac \
  -B \
  -q 0.01

```

   - The `-t` is the treatment or IP (ImmunoPrecipitated) aligned and dup-marked bam file
   - The `-c` is the control or INPUT aligned and dup-marked bam file
   - The `-f` indicates the input file type (BAM paired-end or BAMPE in this case)
   - The `-g` indicates the effective genome size (here precomputed for mm10 and provided as `mm`)
   - The `-n` is the prefix you will give your output files
   - The `-B` indicates that the program should create a BedGraph (`.bdg`) file with the results
   - The `-q` is the FDR cutoff for which to call peaks

## Check files

At the end, you should have something similar to:

```{bash eval = FALSE}
user01@orca01:~/ChIP_tutorial$ ls -lh
total 3.2G
-rw-r--r-- 1 mhirst orca_users 958M Oct  4 15:20 Naive_H3K27ac.final.bam
-rw-r--r-- 1 mhirst orca_users 5.6M Oct  4 15:20 Naive_H3K27ac.final.bam.bai
-rw-r--r-- 1 mhirst orca_users 815M Oct  4 15:57 Naive_H3K27ac_control_lambda.bdg
-rw-r--r-- 1 mhirst orca_users 1.1M Oct  4 15:57 Naive_H3K27ac_peaks.narrowPeak
-rw-r--r-- 1 mhirst orca_users 1.3M Oct  4 15:57 Naive_H3K27ac_peaks.xls
-rw-r--r-- 1 mhirst orca_users 754K Oct  4 15:57 Naive_H3K27ac_summits.bed
-rw-r--r-- 1 mhirst orca_users 310M Oct  4 15:57 Naive_H3K27ac_treat_pileup.bdg
-rw-r--r-- 1 mhirst orca_users 1.1G Oct  4 15:30 Naive_Input.final.bam
-rw-r--r-- 1 mhirst orca_users 5.8M Oct  4 15:30 Naive_Input.final.bam.bai
-rw------- 1 mhirst orca_users   58 Oct  4 05:36 nohup.out
```

## Final Processing

Convert to bigWig and sort (on the Orca server).

```{bash eval = FALSE}
wget http://hgdownload.cse.ucsc.edu/goldenPath/mm10/bigZips/mm10.chrom.sizes

bedtools sort -i Naive_H3K27ac_treat_pileup.bdg > Naive_H3K27ac_treat_pileup.sort.bdg
bedtools sort -i Naive_H3K27ac_control_lambda.bdg > Naive_H3K27ac_control_lambda.sort.bdg

bedGraphToBigWig Naive_H3K27ac_treat_pileup.sort.bdg mm10.chrom.sizes Naive_H3K27ac_treat_pileup.sort.bw
bedGraphToBigWig Naive_H3K27ac_control_lambda.sort.bdg mm10.chrom.sizes Naive_H3K27ac_control_lambda.sort.bw
```

## Transfer all relevant output files to your local computer

Using a different terminal window that is not connected to the server, retrieve the bigWig and narrowPeak files

```{bash eval = FALSE}
scp user01@orca1.bcgsc.ca:/home/<user name>/ChIP_tutorial/*{.bw,.narrowPeak} /your/local/folder
```

Pre-run analysis can be found here:
```{bash eval = FALSE}
/projects/micb405/analysis/ChIP_tutorial
```
You can replace this folder for your own `~/ChIP_tutorial` folder if your script doesn't finish running in time.

Launch IGV and load in the bigWigs and bed (narrowPeak) files.

If you havenâ€™t installed it yet, please get it here IGV download. Make sure you are loading the Mouse (mm10) reference genome by clicking on the drop-down menu on the top left hand corner (Genomes > Load Genome from Server > mm10).

CONGRATULATIONS! You have now completed your first ChIP-seq analysis.  

Questions:

1. What do each of the files outputted by MACS2 represent?
1. Find one H3K27ac peak, either by displaying a text file in your terminal or looking at your files through IGV. You can also use the text file to file a location to jump to within IGV. What is its chromosomal position?
1. How many total peaks were called in this analysis?
1. With these peaks on hand, what is one downstream analysis step you can perform that might rely on a set of called peaks?

## Additional resources {-}

[Harvard Chan Bioinformatics Core ChIP-seq Course](https://hbctraining.github.io/Intro-to-ChIPseq/schedule/2-day.html)

[Peak Calling with MACS2](https://hbctraining.github.io/Intro-to-ChIPseq/lessons/05_peak_calling_macs.html)

<!--chapter:end:Rmds/chip-seq.Rmd-->

# RNA-seq alignment with STAR 

## Slides {-}

```{r, echo = FALSE, out.width = "100%", out.height = "388px"}
knitr::include_graphics("slides/tutorials/star.pdf")
```

You can download the slides for this tutorial below.

```{r echo = FALSE}
downloadthis::download_file(
  path         = here("slides", "tutorials", "star.pdf"),
  output_name  = "star.pdf",
  button_label = "Download slides"
)
```

## STAR workflow

1. Normally, you can download mouse genomic data (in this case, the mm10 FASTA and GTF files) from [Ensembl](https://ftp.ensembl.org/pub/release-84/), but we have prepared this for you in advance at `/projects/micb405/analysis/STAR_tutorial`. Simply create a new working directory and a subdirectory for your STAR index.

    ```{bash eval = FALSE}
    mkdir ~/star && cd ~/star
    mkdir STARIndex
    ```

2. Before you begin your alignment, STAR must generate its own format of index based on the genomic information you provide it with. Generate a STAR index based on the mm10 FASTA and your GTF files, **then proceed to Step 3 while it runs**:

    ```{bash eval = FALSE}
    STAR \
      --runMode genomeGenerate \
      --genomeDir STARIndex \
      --genomeFastaFiles /projects/micb405/analysis/STAR_tutorial/Mus_musculus.GRCm38.dna.primary_assembly.fa \
      --sjdbGTFfile /projects/micb405/analysis/STAR_tutorial/Mus_musculus.GRCm38.84.gtf \
      --sjdbOverhang 49 \
      --runThreadN 16
    ```

3. Download healthy tissue RNAseq FASTQ files from the paper ["An RNA-Seq atlas of gene expression in mouse and rat normal tissues"](https://www.nature.com/articles/sdata2017185). These are located in the associated [ArrayExpress](https://www.ebi.ac.uk/arrayexpress/experiments/E-MTAB-6081/samples/) that can be located in the data citations from the article's NCBI page.

    **Discuss among your group an idea for what you could investigate given the data you have available.** Download the FASTQ files that will allow you to accomplish this. Recall that RNA-seq should indicate relative expression at gene intervals you provide, which can later be quantified and normalized by HTseq & DESeq2.

    ```{bash eval = FALSE}
    # In my case, I'm downloading 4 tissue types from the first individual mouse in order to observe differential gene expression in these areas.
    # These are just an example! 
    wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR213/000/ERR2130640/ERR2130640.fastq.gz
    wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR213/009/ERR2130649/ERR2130649.fastq.gz
    wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR213/003/ERR2130623/ERR2130623.fastq.gz
    wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR213/004/ERR2130614/ERR2130614.fastq.gz
    ```

4. When the STAR index is ready, run STAR, outputting into a separate directory for each sample you wish to align. You can control the output directory with the string provided to `--outFileNamePrefix`. These directories will have to be created by you beforehand - STAR does not create directories for you. Since you won't have time for the whole process to run today, create a script that runs this STAR command for each of your samples and run it in a way so that it won't stop when you exit your shell (Group Assignment part #2 below).

    Be sure that `--readFilesIn`, `--outFileNamePrefix`, and `--outSAMattrRGline` changes for each of your samples

    ```{bash eval = FALSE}
    STAR \
      --genomeDir STARIndex/ \
      --readFilesIn sample1.fastq.gz \
      --outFileNamePrefix sample1/sample1.fastq.gz \
      --runThreadN 8 \
      --limitBAMsortRAM 60000000000 \
      --outSAMattrRGline ID:sample1.fastq.gz SM:sample1.fastq.gz \
      --outBAMsortingThreadN 8 \
      --outSAMtype BAM SortedByCoordinate \
      --outSAMunmapped Within \
      --outSAMstrandField intronMotif \
      --readFilesCommand zcat \
      --chimSegmentMin 20 \
      --genomeLoad NoSharedMemory
    ```

## Group assignment

1. Which samples from the paper did you choose, and what did you want to test with them?
2. Create a script that performs the STAR alignment for your files, avoiding excessive repetition, and can be run so that it is not interrupted when you close your connection with the server.

## Resources

[STAR documentation](https://github.com/alexdobin/STAR/blob/master/doc/STARmanual.pdf)

<!--chapter:end:Rmds/star.Rmd-->

# HTseq and DESeq2 tutorial (Axel Hauduc and adapted from Andrew Wilson)

## Slides {-}

```{r, echo = FALSE, out.width = "100%", out.height = "388px"}
knitr::include_graphics("slides/tutorials/htseq_deseq2.pdf")
```

You can download the slides for this tutorial below.

```{r echo = FALSE}
downloadthis::download_file(
  path         = here("slides", "tutorials", "htseq_deseq2.pdf"),
  output_name  = "htseq_deseq2.pdf",
  button_label = "Download slides"
)
```

## Overview

In this tutorial, we will use DESeq2 to analyze some RNA-Seq data. This tutorial covers how to:

1. Review how to use HTSeq on data you generated on your own
1. Load counts from new data (not your own) into R  
2. Set controls for DESeq2 by changing factor levels  
3. Run sanity checks to ensure your results make biological sense  
4. Filter differential expression tables by Padj and Log2 fold change 

## HTSeq Overview

Normally, you would use HTSeq to create your own count files from the STAR alignments you produced, using the following code:

```
conda create -n my_htseq_env python=2.7.18
conda activate my_htseq_env
conda install htseq

htseq-count \
<sample_id>/<sample_id>.fastq.gzAligned.sortedByCoord.out.bam \
Mus_musculus.GRCm38.84.gtf \
-f bam \
-r pos \
--stranded=no \
> <sample_id>.htseq.out
``` 
...and repeat for every sample.

However, due to constraints on time and available disk space on the server, we will be downloading existing count data from an experiment containing 6 samples grouped into control and treatment categories.

## DESeq2

## Setup

All of the data for this tutorial is located on the Orca1 server in `/projects/micb405/resources/DESeq2_tutorial/`. Use `scp` to copy the entire directory to your computer, and move the directory to where you keep your MICB405 files.

Open RStudio and create a new R project (File > New Project).

Since we want all of our DESeq2 tutorial materials together, click on 'Existing Directory' and choose the `DESeq2_tutorial` directory that you copied from the server. 

### Scripting in R

One of the most important aspects of any bioinformatics is making sure that your code is easy to read and well documented. This means that scripts should be organized and you should use comments to split your code into different sections.

#### Setting up your script

You should always begin an R script with a simple header:

```{r script-header, eval = FALSE}
# Your name
# Title of your script (e.g. MICB405 DESeq2 tutorial)
# Date of your last update
```

The next step of any R script is to load all of the packages that you plan to use:

Be sure to install any packages you have not installed previously on your laptop.

[DESeq2](https://bioconductor.org/packages/release/bioc/html/DESeq2.html) will need to be downloaded with BiocManager:

```{r load-packages, message = FALSE, warning = FALSE, eval = FALSE}
library(DESeq2)
library(tidyverse)
library(pheatmap)
library(RColorBrewer)
```

#### Loading data

It is best practice to collect all input files in a folder (most often called "data") from which files will only be loaded but not overwritten.

Since there are multiple input files, we can use R to automatically generate paths to the files we want to open. All of the data files are in the same directory, so to save us some typing we can assign the path to that directory as a variable.

```{r set-data-directory, message = FALSE, warning = FALSE, eval = FALSE}
dir <- "data"
```

Each filename is generated from the sample name and a common suffix. In order to get a vector of sample names, we can use the `samples.csv` metadata file which we will need also to assign conditions to samples. When you are going to work on your own projects you would enter the filenames and the corresponding conditions yourself into a csv file.

```{r load-metadata, message = FALSE, eval = FALSE}
#  file.path constructs filepath with the correct separator dependent of OS
sample_metadata <- read_csv(file.path(dir,"samples.csv"))
sample_metadata
```

As you can see, the metadata has 2 columns: sample names and conditions. We can generate filenames from `sample_metadata$sample` for each element in the vector (e.g. the first element is TODO: Make back into an r code chunk "` sample_metadata$sample[1]`") by adding the directory TODO: Make back into an r code chunk "` dir`" as prefix and ".htseq.out" as suffix.

```{r generate-filepaths, eval = FALSE}
#  paste0 appends (i.e. adds together) character objects
files <- paste0(file.path(dir, sample_metadata$sample), ".htseq.out")
files
```

Then, as a quick test, you can run `all(file.exists(files))`, which should return `TRUE` to your console if the file paths are correct. First, `files.exists(files)` checks for each filename in `files` if that file  exists and returns `TRUE` or `FALSE`, resulting in a logical vector. If all files are present, then all elements in the logical vector will be `TRUE`.

```{r logical-vector, eval = FALSE}
file.exists(files)
```

 The function `all` only returns `TRUE` if all elements of a logical vector are `TRUE`.
 
```{r check-if-all-is-there, eval = FALSE}
all(file.exists(files))
```

### Running DESeq2

DESeq2 requires that all of your data be in the shape of a SummarizedExperiment object. Since reshaping your input data with your own code would be complicated, there are functions within DESeq2 specific for the source of your counts file, which will prepare your data for use in DESeq2. Since we are using HTSeq to generate our counts data in this class, the function we need is called `DESeqDataSetFromHTSeqCount()`. One of the required arguments is `sampleTable`, which must be provided a data table with columns for `sampleName`, `fileName`, and for any of the independent variables you changed in your experiment (eg. sex, treatment, tissue). For that purpose, we can quickly build `sample_df` from the loaded metadata `sample_metadata` and the vector of file paths `files`, that we defined earlier.

The other required argument for `DESeqDataSetFromHTSeqCount()` is `design`. The design argument is a formula, designated by the tilde symbol `~`, and identifies which independent variable(s) you want to investigate. In our data, we have only a single independent variable, i.e. "condition". Experiments with multifactorial experimental design are a lot more complicated to investigate.

```{r deseq-dataset, eval = FALSE}
# Creates a new data table with the variables sampleName, fileName and condition
sample_df <- data.frame(sampleName = sample_metadata$sample,
                        fileName = files,
                        condition = sample_metadata$condition)

ddsHTSeq <- DESeqDataSetFromHTSeqCount(sampleTable = sample_df,
                                       design = ~ condition)
```

In a DESeq data set, the design variables are stored as a factor (a data type in R), which by default are organized alphabetically. Without further information, DESeq2 will use the condition that comes first alphabetically as reference. If you would be using "treated" and "untreated", for example, DESeq2 would incorrectly assume that the "treated" samples are the reference. To prevent any confusion, we can set the name used as reference explicitly with the argument `ref` inside of the `relevel()` function.

```{r set-controls, eval = FALSE}
## Set control condition using the relevel function
ddsHTSeq$condition <- relevel(ddsHTSeq$condition, ref = "control")
```

Now that the reference level has been set, we can run DESeq2 on the dataset.
```{r deseq2, message = FALSE, warning = FALSE, eval = FALSE}
dds <- DESeq(ddsHTSeq)
```

### Sample Clustering
Because the distribution of RNA Seq data is highly skewed, with a few high abundance genes and many low abundance genes, it is helpful to transform our data to visualize clustering. DESeq2 comes with a function `rlog()`, which log-transforms your count data. After transformation, we can use PCA to identify which samples are more similar and if they group by one or more of the independent variables (in our case, we  have only a single variable that can take "control" or "treated").

```{r pca, eval = FALSE} 
rld <- rlog(dds)
plotPCA(rld, intgroup = "condition")
```

A distance matrix is another way to examine how samples cluster. The code chunk below calculates distances between each of the samples and then maps a colour palette (defined with `colorRampPalette`) onto it, so that more similar samples display darker colours.

```{r distance-matrix, eval = FALSE}
sample_dists <- dist(t(assay(rld)))
sample_dist_matrix <- as.matrix(sample_dists)
colnames(sample_dist_matrix) <- NULL
colours <- colorRampPalette(rev(brewer.pal(9, "Blues")))(255)
pheatmap(sample_dist_matrix,
         clustering_distance_rows = sample_dists,
         clustering_distance_cols = sample_dists, 
         col = colours)
```

If the samples don't cluster together the way that you would expect them to, check for batch effects.

### Filtering and data export

Since our samples cluster according to the independent variable, we can move forward with our analysis. To examine our results, we can to save them to our global environment. DESeq2 calculates results for all combinations of levels in your experimental design. The `resultsNames()` function tells us the name of each result that DESeq2 has calculated, so we can choose which one to look at by specifying "name = " in `results()`. The `results()` function will output a matrix of all genes, including those with similar or differential expression between the two conditions.

```{r generate-result-table, eval = FALSE}
resultsNames(dds)
res <- results(dds, name = "condition_treatment_vs_control")
head(res)
```

Since we are only interested in genes which are differentially expressed, we can use the `subset()` function from base R to only keep results with an adjusted P value < 0.05, and then save the results to a CSV file.

```{r subset-and-export-results, eval = FALSE}
significant_res <- subset(res, padj < 0.05)
write.csv(as.data.frame(significant_res), file = "treat_vs_control_05.csv")
```

If you want more information about DESeq2, you can access the package vignette by running:

```{r vignette, eval = FALSE}
vignette("DESeq2")
```

Now that you have a distance matrix and a list of significant genes, what is one downstream analysis you could do that could inform you about a biological difference between the two treatment groups? 

## Additional resources

Original paper: [An RNA-Seq atlas of gene expression in mouse and rat normal tissues](https://www.nature.com/articles/sdata2017185)

[Data source from original paper](https://www.ebi.ac.uk/arrayexpress/experiments/E-MTAB-6081/samples/)

[STAR documentation](https://github.com/alexdobin/STAR/blob/master/doc/STARmanual.pdf)

[HTSeq-count documentation](https://htseq.readthedocs.io/en/release_0.11.1/count.html)

[DESeq2 vignette](https://www.bioconductor.org/packages/devel/bioc/vignettes/DESeq2/inst/doc/DESeq2.html)

<!--chapter:end:Rmds/htseq_deseq2.Rmd-->

# (PART) R lectures {-}

<!--chapter:end:Rmds/part_r_lectures.Rmd-->

# Introduction to R and RStudio

## Slides

```{r echo=FALSE}
xaringanExtra::embed_xaringan(url = "slides/lectures/intro_r.html",
                              ratio = "4:3")
```

You can download the slides for this tutorial below.

```{r echo = FALSE}
downloadthis::download_file(
  path         = here("slides", "lectures", "intro_r.html"),
  output_name  = "7_1_introduction_to_r_and_rstudio",
  button_label = "Download slides"
)
```

## Learning outcomes

By the end of this tutorial you should be able to:

-   Identify the different components of RStudio.
-   Declare variables in R.
-   Identify common data types and structures used in R.
-   Recognize and use functions.
-   Install and load R packages.
-   Interpret documentation for functions and packages.

::: rmdtip
You should have both [R and RStudio] installed. Please open RStudio and work along the examples.
:::

```{r include = FALSE}
library(tidyverse)
library(pheatmap)
library(BiocManager)
library(DESeq2)
```

```{r child = here("Rmds", "child_Rmds", "rstudio.Rmd")}
```

## Setup

```{r child = here("Rmds", "child_Rmds", "rproj.Rmd")}
```

```{r child = here("Rmds", "child_Rmds", "packages.Rmd")}
```

In the following tutorials, we will be using the tidyverse, pheatmap, BiocManager, and DESeq2 packages. tidyverse contains a versatile set of functions designed for easy manipulation of data. pheatmap visualizes gene expression levels and groups genes with similar patterns across treatments. BiocManager allows us to install packages from the Bioconductor platform (see below). Finally, DESeq2 compares gene expression across treatments.

These packages are distributed on two different platforms, CRAN (Comprehensive R Archive Network) and Bioconductor (think of those platforms as two different app stores for R packages).

```{r child = here("Rmds", "child_Rmds", "cran.Rmd")}
```

```{r eval = FALSE}
install.packages(c("tidyverse", "pheatmap", "BiocManager"))
```

```{r child = here("Rmds", "child_Rmds", "bioconductor.Rmd")}
```

```{r eval = FALSE}
BiocManager::install("DESeq2")
```

```{r child = here("Rmds", "child_Rmds", "loading_packages.Rmd")}
```

```{r eval = FALSE}
library(tidyverse)
```

```{r child = here("Rmds", "child_Rmds", "r_scripts.Rmd")}
```

```{r child = here("Rmds", "child_Rmds", "data_types.Rmd")}
```

```{r child = here("Rmds", "child_Rmds", "functions.Rmd")}
```

Sometimes, you want to confirm the data type (see above) of a variable, and you can do so by using the `typeof()` function.

```{r}
typeof(microbiology_is_awesome)
```

```{r child = here("Rmds", "child_Rmds", "help.Rmd")}
```

:::rmdwarning
```{r child = here("Rmds", "child_Rmds", "exercises", "help_for_log_ex.Rmd")}
```
:::

::: rmdwarning
You have never used the function `inner_join()` of the dplyr package before. Take a look at the help documentation for `inner_join()` in RStudio (**Hint**: You can only look at the documentation for functions of packages you have loaded). In the list below, identify all of the arguments of the function that are mandatory and have to be specified by you.

-   `x`
-   `y`
-   `by`
-   `copy`
-   `suffix`
-   `keep`

What data type is accepted by the `keep` argument?
:::

```{r child = here("Rmds", "child_Rmds", "data_structures.Rmd")}
```

<!--chapter:end:Rmds/intro_r.Rmd-->

# DESeq2: count normalization

This tutorial will use many tools found in the tidyverse package to reproduce how DESeq2 calculates the size factors to normalize the count data.

```{r include = FALSE}
counts <- read_csv(here("data", "pasilla_gene_counts.csv"))
```

## Slides

```{r echo=FALSE}
xaringanExtra::embed_xaringan(url = "slides/lectures/count_normalization.html",
                              ratio = "4:3")
```

You can download the slides for this tutorial below.

```{r echo = FALSE}
downloadthis::download_file(
  path         = here("slides", "lectures", "count_normalization.html"),
  output_name  = "7_2_deseq2_count_normalization",
  button_label = "Download slides"
)
```

## Learning outcomes

By the end of this tutorial, you should be able to:

- Define the challenges of differential analysis.
- Apply different count normalization strategies.
- Reproduce count normalization of DESeq2 in R using tidyverse

::: rmdtip
You should have both [R and RStudio] installed. Please open RStudio and work with the examples by editing your `` `r params$project`.R`` script.
:::

## Setup

## R packages

We will work with the tidyverse package so make sure that it is loaded.

```{r eval = FALSE}
library(tidyverse)
```

## Data

The RNAseq data were collected from *Drosophila melanogaster* in which the splicing regulator Pasilla was depleted by RNAi (`treated`) or not (`untreated`) [@brooks2011]. Download the data set `pasilla_gene_counts.csv` below and place it in your `r params$project` directory.

```{r echo = FALSE}
downloadthis::download_file(
  path         = here("data", "pasilla_gene_counts.csv"),
  button_label = "Download Pasilla data set"
)
```

## Working with data frames

### Loading tabular data from a file

Data frames can be loaded into R using many different functions. In the tidyverse, they are called `read_*()`. For example, let's inspect `Spasilla_gene_counts.cs` which contains a table using Notepad (Windows) or TextEdit (macOS).

```{r echo = FALSE, comment = NA}
cat(paste(read_lines(here("data", "pasilla_gene_counts.csv"),
                     n_max = 4),
          collapse = "\n"),
    fill = TRUE)
```

You will notice that the first row is different from all the others, and it actually contains column names for our data. Each row after the first one makes up a row in the table. You will also notice that commas separate values in each row. The first element in each row (i.e. before the first comma) makes up the value in the first column, each second element the value in the second column, and so on. Such a file is described as having comma-separated values. For that reason, files with that format often have the extension `.csv`.

We can load `pasilla_gene_counts.csv` into R with `read_csv` for a comma-separated file and specify the arguments that describe our data as follows:

-   `file`: the name of the file you want to load as a character data type, i.e. wrapped in quotes.
-   `col_names`: can take either the value `TRUE` or `FALSE` and tells R if the first row contains column names.

```{r eval = FALSE}
read_csv(file = "pasilla_gene_counts.csv", col_names = TRUE)
```

The data are printed to the console in R.

### Save data in the environment

Since we want to do more with our data after reading it in, we need to save it as a variable in R with the `<-` operator. You can choose to name the object whatever you like, though this module assumes the names used below.

```{r eval = FALSE}
counts <- read_csv(file = "pasilla_gene_counts.csv", col_names = TRUE)
```

### Data exploration

Let's explore the data that we've imported into R by clicking on the triangle of the `counts` in the Environment tab in the top-right pane. You can see each column and its data type in a long list.

```{r echo = FALSE}
str(counts, give.attr = FALSE)
```

If we directly click on `counts` in our Environment tab on the top-right pane, we can view the table. Row and column names are in grey boxes, and data is in white boxes.

Using different functions, we can look at the dimensions of our data, number of rows, and number of columns:

```{r}
# Number of rows followed by number of columns
dim(counts)

# Number of rows
nrow(counts)

# Number of columns
ncol(counts)
```

We can list the column names using `colnames()`:

```{r}
colnames(counts)
```

We can call specific columns using the `$` notation, which is useful for calling an entire column, regardless of its length. For example, calling the `untreated1` column:

```{r}
head(counts$untreated1)
```

## Data wrangling with dplyr

A popular package for data wrangling is *dplyr* in the tidyverse. This package is so good at what it does and integrates so well with other popular tools like *ggplot2* that it has rapidly become the de-facto standard.

dplyr code is very readable because all operations are based on using dplyr functions or *verbs* (`mutate()`, `filter()`, ...). Each verb works similarly:

- Input data frame in the first argument.
- Other arguments can refer to variables as if they were local objects (i.e. you do not need to use quotes around a `"<variable>"` name and can just directly refer to it with `<variable>`).
- Output is another data frame.

### Creating of modifying variables with `mutate()`

Use `mutate()` to apply a transformation to some variable and assign it to a new variable. As the first step for determining the size factor for each sample, DESeq2 calculates the natural logarithm of each gene count.

```{r}
counts_single_log_column <- mutate(counts, log_untreated1 = log(untreated1))
```

If we want to determine the logarithm for each variable, then the code becomes quickly tedious and repetitive.

```{r}
counts_two_log_column <- mutate(counts,
                                log_untreated1 = log(untreated1),
                                log_untreated2 = log(untreated2))
```

Instead, we can use `across()` to apply the same function to each variable and overwrite it. We can select a range of variables with `<first_variable>:<last_variable>`.

```{r}
log_counts <- mutate(counts, across(treated1:untreated4, log))
```

Alternatively, we can define what variables the function NOT to apply to with `!`. In our example, we want to skip `gene_id`.

```{r}
log_counts <- mutate(counts, across(!gene_id, log))
```

### Summarizing across variables with `rowwise()` and `c_across()`

We can group our data with `rowwise()` and then use `c_across()`,to calculate the mean across multiple columns (step 2 of DESeq2 normalization). The mean of the log values is not as sensitive to outliers compared to the means of the count values themselves.

```{r}
log_counts_grouped <- rowwise(log_counts)
geometric_means <- mutate(log_counts_grouped,
                          mean = mean(c_across(treated1:untreated4)))
```

### Piping with `%>%`

Because of the basic dplyr verb syntax, these functions lend themselves to piping, i.e. taking the output of one function and passing it in as a (by default the first) argument of the following function.

```{r}
geometric_means <- log_counts %>% 
  rowwise() %>% 
  mutate(gm = mean(c_across(treated1:untreated4)))
```

### Remove groupings with `ungroup()`

Because we are finished with the row-wise operation, we should remove the grouping with `ungroup()`.

```{r}
geometric_means <- log_counts %>% 
  rowwise() %>% 
  mutate(gm = mean(c_across(treated1:untreated4))) %>% 
  ungroup()
```

### `filter()`

Conditional statements and logical operators are essential when working with data in R. You can use `filter()` to select specific rows based on a logical condition of a variable. For quick reference, here are the most commonly used statements and operators.

R code     | meaning
---------- | ---------------
`==`       | equals
`< or >`   | less/greater than
`<= or >= `| less/greater than or equal to
`%in%`     | in
`is.na()`  | is missing (NA)
`!`        | not (as in not equal to `!=`)
`&`        | and
`|`        | or

As step 3 of DESeq2 count normalization, all rows that contain an infinite value for their geometric mean are removed. `is.infinite()` would return rows where the variable is infinite. `!is.infinite()` returns rows where it is not.

```{r}
geometric_means <- log_counts %>% 
  rowwise() %>% 
  mutate(gm = mean(c_across(treated1:untreated4))) %>% 
  ungroup() %>% 
  filter(!is.infinite(gm))
```

In step 4 of DESeq2 of count normalization, the geometric mean is subtracted of each $log(count) - log(gm) = log\left(\frac{{counts}}{{gm}}\right)$, i.e. it looks at the ratio of the sample to the mean.

```{r}
ratio <- geometric_means %>% 
  mutate(across(treated1:untreated4, ~ . - gm))
```

### Column-wise operation with `summarise()`

Now that we know the ratio of sample to mean, we select the `median()` across all remaining genes for each sample. To calculate a summary statistic for a column, we use `summarise()`. Finally, we take the inverse logarithm to receive the size factor.

```{r}
size_factors <- ratio %>% 
  summarise(across(treated1:untreated4, ~ exp(median(.))))
```

<!--chapter:end:Rmds/count_normalization.Rmd-->

# (APPENDIX) Appendix {-}

<!--chapter:end:Rmds/appendix.Rmd-->

# Bash cheat sheet

+-----------------------------------+--------------------------------------------------------------------------------------------------------------------------------+
| Command                           | Description                                                                                                                    |
+===================================+================================================================================================================================+
| `cd`                              | Change directory                                                                                                               |
|                                   |                                                                                                                                |
|                                   | `..` to parent directory                                                                                                       |
|                                   |                                                                                                                                |
|                                   | `~` to home directory                                                                                                          |
|                                   |                                                                                                                                |
|                                   | `-` to last visited directory                                                                                                  |
+-----------------------------------+--------------------------------------------------------------------------------------------------------------------------------+
| `ls`                              | List files in directory                                                                                                        |
|                                   |                                                                                                                                |
|                                   | `-l` as list                                                                                                                   |
+-----------------------------------+--------------------------------------------------------------------------------------------------------------------------------+
| `ssh <username>@<server address>` | Make a secure connection to server                                                                                             |
+-----------------------------------+--------------------------------------------------------------------------------------------------------------------------------+
| `pwd`                             | Print working directory                                                                                                        |
+-----------------------------------+--------------------------------------------------------------------------------------------------------------------------------+
| `less <filename>`                 | Look at text file                                                                                                              |
|                                   |                                                                                                                                |
|                                   | <kbd>h</kbd>elp for navigation                                                                                                 |
|                                   |                                                                                                                                |
|                                   | <kbd>q</kbd>uit                                                                                                                |
+-----------------------------------+--------------------------------------------------------------------------------------------------------------------------------+
| `scp <source> <target>`           | Copy files between local and server                                                                                            |
|                                   |                                                                                                                                |
|                                   | Give file path on server as `<user>@<server address>:<file path>`                                                              |
+-----------------------------------+--------------------------------------------------------------------------------------------------------------------------------+
| `exit`                            | Close session (either to server or to local terminal)                                                                          |
+-----------------------------------+--------------------------------------------------------------------------------------------------------------------------------+
| `history`                         | Get list of command history                                                                                                    |
|                                   |                                                                                                                                |
|                                   | Follow with `!<number>` to execute that corresponding command again                                                            |
|                                   |                                                                                                                                |
|                                   | `-a <filename>` save a history of your current session to a text file to document your work                                    |
+-----------------------------------+--------------------------------------------------------------------------------------------------------------------------------+
| `man <command>`                   | Unfortunately, it is impossible to predict how to get help for a `<command>`, try any one of these until one works             |
|                                   |                                                                                                                                |
| `help <command>`                  |                                                                                                                                |
|                                   |                                                                                                                                |
| `<command> --help`                |                                                                                                                                |
+-----------------------------------+--------------------------------------------------------------------------------------------------------------------------------+
| `screen`                          | Create a virtual terminal that continues to run jobs even when your connection to the server terminates                        |
|                                   |                                                                                                                                |
|                                   | `-S <session name>` create a session with `<session name>`                                                                     |
|                                   |                                                                                                                                |
|                                   | `-ls` list all running sessions                                                                                                |
|                                   |                                                                                                                                |
|                                   | `-r <session name>` resume a session                                                                                           |
|                                   |                                                                                                                                |
|                                   | To leave a session but keep it running, first <kbd>ctrl</kbd>+<kbd>a</kbd>ctivate commands to `screen`, then <kbd>d</kbd>etach |
+-----------------------------------+--------------------------------------------------------------------------------------------------------------------------------+

: (\#tab:commands) Shell cheat sheet

<!--chapter:end:Rmds/cheat_sheet.Rmd-->

# Terminal keyboard shortcuts

```{r child = here("Rmds", "child_Rmds", "terminal_keyboard_shortcuts.Rmd")}
```

<!--chapter:end:Rmds/terminal_keyboard_shortcuts.Rmd-->

# `less` and `man` navigation

```{r eval = TRUE, child = here("Rmds", "child_Rmds", "less_navigation.Rmd")}
```

<!--chapter:end:Rmds/less.Rmd-->

# Resources
These are OPTIONAL resources if you want to dive into course topics in more depth. You will not be tested on content that does not appear in the lecture material.

## Unix
-   [Software Carpentry - UNIX Novice Course](https://swcarpentry.github.io/shell-novice/)
-   [sandbox.bio Online Shell & Tutorials](https://sandbox.bio/)

## R
-   [R for Data Science](https://r4ds.had.co.nz/index.html)
-   [Software Carpentry - Programming in R](http://swcarpentry.github.io/r-novice-inflammation)
-   [Software Carpentry - R for Reproducible Scientific Analysis](http://swcarpentry.github.io/r-novice-gapminder)

## Bioinformatics
-   [Bioinformatics Data Skills (Unix, R, & much, much more)](https://ebookcentral.proquest.com/lib/ubc/reader.action?docID=3564550)

<!--chapter:end:Rmds/resources.Rmd-->

# References

<!--chapter:end:Rmds/references.Rmd-->

