# Background information

## Project information

Marine oxygen minimum zones (OMZ) are widespread areas of low dissolved oxygen (DO) in subsurface waters. Climate change resulting in increased stratification and reduced oxygen solubility in warming waters leads to an expansion of OMZ. Consequently as oxygen levels decline, the microbial communities inhabiting OMZ waters shift their metabolisms to utilize alternative terminal electron acceptors. This results in the production of climate active trace gases such as nitrous oxide (N2O) and methane (CH4). OMZs provide useful environmental contexts in which to study coupled biogeochemical cycling through microbial metabolic networks. By combining 'omics data with biogeochemical data from OMZs it becomes possible to evaluate regulatory and response dynamics of microbial communities to changing DO levels. In this study, Saanich inlet, a seasonally anoxic fjord on the coast of Vancouver Island British Columbia, was used as a model ecosystem to study microbial community responses to changing DO levels.

## Data collection

Multi-omic sequence information (DNA, RNA and proteins) from Saanich Inlet along with geochemical parameter information was collected over many years prodicing a time series record of recurring water column stratificaiton and deep water renewal. For this tutorial, 90 metagenomes spanning water colum redix gradients in space and time, totalling 4.1 TB of cleaned reads or 16.2 GB of assembled data, were prepared. [(a) Oxygen concentration contour for CTD data (February 2008 onward)35 indicating 16 sampling depths for water column geochemistry and high-resolution (HR) DNA samples for SSU libraries (small black dots) and six major depths for large volume (LV) samples for meta-genomics, -transcriptomics, -proteomics and LV SSU libraries (large black dots). (b) Sample inventory from February 2006 to October 2014 indicating multi-omic datasets included in this manuscript (solid black), in previous publications (gray) and accompanying datasets currently undergoing processing and analysis (open gray).](data_multi-omic_samples.png)

Please refer [Hawley, A. K. et al. (2017)](https://www.nature.com/articles/sdata2017160) and [Torres-BeltrÃ¡n, M. et al. (2017)](https://www.nature.com/articles/sdata2017159)

## Data availability

The following files are available on Orca.

```
# Directory with all files
/projects/micb405/resources/metagenomics_data

# Raw reads by depth
/projects/micb405/resources/metagenomics_data/SI072_100m_raw_reads.fastq.gz
/projects/micb405/resources/metagenomics_data/SI072_120m_raw_reads.fastq.gz
/projects/micb405/resources/metagenomics_data/SI072_200m_raw_reads.fastq.gz

# Deduplicated set of contigs from the combined 100, 120 and 200 m datasets
/projects/micb405/resources/metagenomics_data/SI072_Dedupe_Contigs.fa

# Directory with bins refined with anvi'o and CheckM table
/projects/micb405/resources/metagenomics_data/SI072_Bins_Refined_final

# Directory with taxonomic classification of refined bins by GTDB-Tk
/projects/micb405/resources/metagenomics_data/SI072_Bins_Ref_5_GTDB_r95

# Directory with taxonomic classification of refined bins by Centrifuge
/projects/micb405/resources/metagenomics_data/Import_To_Contig_DB_files/Centrifuge_Taxonomy

# Directory with Prokka output for MAGs
/projects/micb405/resources/metagenomics_data/Import_To_Contig_DB_files/Prokka_Functions

# BAM files and indices for data
/projects/micb405/resources/metagenomics_data/bams
```
## Preliminary data processing

### Quality filtering of reads
Quality filtering was performed with Trimmomtic (v.0.35) developed by [Bolger et al., (2014)](https://doi.org/10.1093/bioinformatics/btu170). Please check [Trimmomatic documentation](http://www.usadellab.org/cms/?page=trimmomatic) for more details on the use of Trimmomatic. The following parameters were used for Trimmomatic, with the other parameters as defaults:
LEADING:3 
TRAILING:3 
SLIDINGWINDOW:4:15 
MINLEN:36

### Assembly
Megahit (v.1.1.3), developed by [Li et al., (2015)](https://doi.org/10.1093/bioinformatics/btv033), was used with the following parameters (the rest of the parameters being default):
--k-min 27 
--k-step 10 
--min-contig-len 500

Please review [megahit documentation](https://github.com/voutcn/megahit/wiki/Assembly-Tips) for more details on the usage

### Deduplication
[BBtools](https://jgi.doe.gov/data-and-tools/bbtools/) and dedupe.sh script were used to deduplicate contigs
Please visit the [deduplication guide](https://jgi.doe.gov/data-and-tools/bbtools/bb-tools-user-guide/dedupe-guide/#:~:text=Dedupe%20was%20written%20to%20eliminate,of%20substitutions%20or%20edit%20distance.&text=Dedupe%20is%20primarily%20designed%20to%20handle%20these%20situations.) from the Joint Genome Institute for more information.

The following command generates deduplicated assemblies
```
dedupe.sh in=SI072_Combined_contigs.fa out=SI072_Dedupe_Contigs.fa outd=duplicates.fa
```

### Mapping 
Mapping was performed with bwa-mem ([Li H. (2013)](https://arxiv.org/pdf/1303.3997.pdf)). Please check [bwa-mem documentation](https://github.com/lh3/bwa) for details on the tool's usage

```
bwa mem -t 4 SI072_Dedupe_Contigs.fa SI072_(depth_here)m_raw_reads.fastq.gz | samtools view -b | samtools sort -o SI072_(depth_here)m_Dedupe_contigs_sorted.bam

samtools view -b -F 4 SI072_(depth_here)m_Dedupe_contigs_sorted.bam > SI072_(depth_here)m_Dedupe_contigs_sorted.mapped.bam

```
